{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook adapted from Jeremy P. Howard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, randn\n",
    "from matplotlib import pyplot as plt, animation, rcParams, rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(alpha,beta,x): return alpha+beta*x\n",
    "rc('animation',html='html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "beta_true = 6.0\n",
    "alpha_true = 3.0\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Square error is square of difference between predicted and actual data values</p>\n",
    "Define predicted: ${\\hat y} $ and actual:${y}$ \n",
    "<p>Loss is:</p>\n",
    "$({\\hat y}-{y})^2$\n",
    "<p>Previous definition y as linear_function</p>\n",
    "${\\hat y=\\alpha + \\beta x}$\n",
    "<p>To minmize the Loss we take the derivative of the loss wrt alpha and beta, not x. x and y are datapoints\n",
    "which is our dataset we are trying to fit to. The derivative of y is the sum of the partial derivatives of the dependent variables\n",
    "which in this case are alpha and beta. </p>\n",
    "<p></p>\n",
    "$derivative(loss)=\\cfrac{\\partial(loss)}{\\partial \\alpha} + \\cfrac{\\partial(loss)}{\\partial \\beta}$\n",
    "<p></p>\n",
    "$\\cfrac{\\partial(loss)}{\\partial \\alpha}=\\cfrac{(\\hat y - y)^2}{\\partial \\alpha}=\\cfrac{2*(\\hat y-y)*\\partial(\\hat y - y)}{\\partial \\alpha}$\n",
    "<p>Using:</p>\n",
    "$\\cfrac{\\partial \\hat y}{\\partial \\alpha}=1$ \n",
    "<p>and</p>\n",
    "$\\cfrac{\\partial y}{\\partial \\alpha}=0$\n",
    "<p>the derivative of loss wrt alpha for y-hat is:</p>\n",
    "$\\cfrac{\\partial (loss)}{\\partial \\alpha}=2(\\hat y-y)$\n",
    "<p>The derivative of loss WRT beta starts as:</p>  \n",
    "$\\cfrac{\\partial(loss)}{\\partial \\beta}=\\cfrac{(\\hat y - y)^2}{\\partial \\beta}=\\cfrac{2*(\\hat y-y)}{\\partial \\beta}*\\cfrac{\\partial(\\hat y -y)}{\\partial \\beta}$\n",
    "<p></p>\n",
    "$\\cfrac{\\partial \\hat y}{\\partial \\beta}=x$ and\n",
    "$\\cfrac{\\partial y}{\\partial \\beta}=0$\n",
    "<p>the derivative of loss wrt beta for y-hat is:</p>\n",
    "$\\cfrac{\\partial (loss)}{\\partial \\beta}=2x*(\\hat y-y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_and_beta():\n",
    "    global alpha, beta\n",
    "    y_predicted = linear_function(alpha,beta,x)\n",
    "    derivative_of_loss_wrt_alpha = 2*(y_predicted-y)\n",
    "    derivative_of_loss_wrt_beta = x* derivative_of_loss_wrt_alpha\n",
    "    alpha = alpha - learning_rate*derivative_of_loss_wrt_alpha.mean()\n",
    "    beta = beta - learning_rate*derivative_of_loss_wrt_beta.mean()\n",
    "# i is the frame nuber!\n",
    "#https://math.unice.fr/~hheumann/Tutorial/_build/intro_matplotlib.html\n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,frames=np.arange(0,250),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out (or delete) the following command, which shows the animation in Jupyter Notebook, \n",
    "# if you want the mp4 saved to disk to be correctly animated. \n",
    "# (Alternatively, you can reset the values of alpha and beta to their initial values \n",
    "# after this command.)\n",
    "\n",
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('linear_animation.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done linear animation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=\\gamma x^2 + \\beta x + \\alpha$\n",
    "<p></p>\n",
    "$\\cfrac{\\partial(loss)}{\\partial \\gamma}=2x^2(\\hat y-y)$\n",
    "$\\cfrac{\\partial(loss)}{\\partial \\beta}=2x(\\hat y -y)$\n",
    "$\\cfrac{\\partial(loss)}{\\partial \\alpha}=2(\\hat y-y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Squared Function</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It doesnt make sense to use a quadratic to approximate a linear function,so change the dataset to \n",
    "#be quadratic\n",
    "def square_function(gamma, beta, alpha,x):\n",
    "    return gamma*x**2 + beta*x + alpha\n",
    "\n",
    "beta_true = 2.0\n",
    "alpha_true = 2.0\n",
    "gamma_true = 10.0\n",
    "\n",
    "n = 300\n",
    "x = random(n)\n",
    "y = square_function(gamma_true,beta_true,alpha_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "gamma = 2.\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_squared():\n",
    "    global gamma, alpha, beta\n",
    "    y_predicted = square_function(gamma,beta,alpha,x)\n",
    "    derivative_of_loss_wrt_gamma = 2*x**2*(y_predicted-y)\n",
    "    derivative_of_loss_wrt_alpha = 2*(y_predicted-y)\n",
    "    derivative_of_loss_wrt_beta = x* derivative_of_loss_wrt_alpha\n",
    "    alpha = alpha - learning_rate*derivative_of_loss_wrt_alpha.mean()\n",
    "    beta = beta - learning_rate*derivative_of_loss_wrt_beta.mean()\n",
    "    gamma = gamma - learning_rate * derivative_of_loss_wrt_gamma.mean()\n",
    "\n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = square_function(gamma,beta,alpha,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_squared()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,250),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=30, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('squared_animation.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done squared animation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>GD w/momentum</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, randn\n",
    "from matplotlib import pyplot as plt, animation, rcParams, rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(alpha,beta,x): return alpha+beta*x\n",
    "rc('animation',html='html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "beta_true = 6.0\n",
    "alpha_true = 3.0\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "learning_rate = 0.01\n",
    "initial_velocity=0.1\n",
    "\n",
    "mom=0.9\n",
    "v_alpha = 0\n",
    "v_beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_and_beta():\n",
    "    global alpha, beta,v_alpha,v_beta\n",
    "    #more efficient code w/o the loop bc it is vectorized\n",
    "    y_predicted = linear_function(alpha,beta,x)\n",
    "    derivative_of_loss_wrt_alpha = 2*(y_predicted-y)\n",
    "    derivative_of_loss_wrt_beta = x * derivative_of_loss_wrt_alpha\n",
    "    v_alpha = mom*v_alpha - np.mean(learning_rate*derivative_of_loss_wrt_alpha)\n",
    "    v_beta = mom*v_beta - np.mean(learning_rate*derivative_of_loss_wrt_beta)\n",
    "    alpha = alpha + v_alpha\n",
    "    beta = beta + v_beta\n",
    "    \n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,250),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('GD_w_mom_animation.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Test Vectorized mean</h6>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonvectorized version. do not do this. \n",
    "def update_alpha_and_beta():\n",
    "    global alpha, beta,v_alpha,v_beta\n",
    "    #more efficient code w/o the loop bc it is vectorized\n",
    "    for i in range(n):\n",
    "        y_predicted = linear_function(alpha,beta,x[i])\n",
    "        derivative_of_loss_wrt_alpha = 2*(y_predicted-y[i])\n",
    "        derivative_of_loss_wrt_beta = x[i] * derivative_of_loss_wrt_alpha\n",
    "        v_alpha = mom*v_alpha - (1/n)*learning_rate*derivative_of_loss_wrt_alpha\n",
    "        v_beta = mom*v_beta - (1/n)*learning_rate*derivative_of_loss_wrt_beta \n",
    "    alpha = alpha + v_alpha\n",
    "    beta = beta + v_beta\n",
    "    \n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the code I see on blogs are wrong. Only JH got it right. \n",
    "import time\n",
    "test = np.random.rand(100000000)\n",
    "\n",
    "def vec_mean():\n",
    "    start_time = time.time()\n",
    "    test.mean()\n",
    "    end_time = time.time()\n",
    "    print(\"vec mean time:\",end_time-start_time)\n",
    "def nonvec_mean():\n",
    "    start_time = time.time()\n",
    "    sum=0.\n",
    "    for i in range(len(test)):\n",
    "        sum += test[i]\n",
    "    mean = sum/len(test)\n",
    "    end_time = time.time()\n",
    "    print(\"nonvec mean time:\",end_time-start_time)\n",
    "    \n",
    "vec_mean()\n",
    "nonvec_mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Gradient Descent with Nesterov</h6>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Loss function replaces theta with theta_tilde </p>\n",
    "$L(f(x^{(i)}); \\tilde \\theta,y^{(i)})$\n",
    "<p>where theta_tilde is theta with a constant times velocity</p>\n",
    "$\\tilde \\theta = \\theta + \\alpha v $\n",
    "<p>To calculate the gradient g we need to compute the partial derivatives of the loss function</p>\n",
    "$loss=(\\hat y -\\tilde y)$\n",
    "<p>where we convert theta to theta_tilde by replacing all the parameters in the linear equation with tilde versions</p>\n",
    "$y(\\tilde \\theta) = \\tilde A x + \\tilde B$ compared to before $y(\\theta) = Ax+b$\n",
    "<p>We changed notation from the earlier convention to avoid confusion with the IG defition of alpha as the\n",
    "momentum coefficient vs. alpha defined as a theta parameter as before</p>\n",
    "$Loss(\\tilde \\theta)=(\\hat y -y(\\tilde \\theta))^2$\n",
    "<p>Taking the partial derivatives of the loss WRT A and B which is the equivalent of taking the partials WRT alpha and beta earlier</p>\n",
    "$\\cfrac{\\partial Loss(\\tilde \\theta)}{\\partial A }=2(\\hat y -y(\\tilde \\theta)) \\cfrac{\\partial (\\tilde y - y(\\tilde \\theta))}{\\partial A}=2(\\hat y -y(\\tilde \\theta))(-x) $\n",
    "<p></p>\n",
    "$\\cfrac{\\partial Loss(\\tilde \\theta)}{\\partial B }= 2(\\hat y -y(\\tilde \\theta)) \\cfrac{\\partial (\\tilde y - y(\\tilde \\theta))}{\\partial B}=2(\\hat y -y(\\tilde \\theta))(-1) $\n",
    "<p>Converting the expectations to mean</p>\n",
    "$\\cfrac{\\partial Loss(\\tilde \\theta)}{\\partial A} = \\cfrac{-2}{N}\\sum_{1}^{N}x(\\hat y - y(\\theta))=\\cfrac{2}{N}\\sum_{1}^{N}x(y(\\theta)-\\hat y)$\n",
    "<p></p>\n",
    "$\\cfrac{\\partial Loss(\\tilde \\theta)}{\\partial B }= \\cfrac{-2}{N}\\sum_{1}^{N}(\\hat y - y(\\theta))=\\cfrac{2}{N}\\sum_{1}^{N}(y(\\theta)-\\hat y)$\n",
    "<p></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, randn\n",
    "from matplotlib import pyplot as plt, animation, rcParams, rc\n",
    "\n",
    "def linear_function(alpha,beta,x): \n",
    "    return alpha+beta*x\n",
    "rc('animation',html='html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "beta_true = 6.0\n",
    "alpha_true = 3.0\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "mom=0.9\n",
    "v_alpha = 0\n",
    "v_beta = 0\n",
    "alpha_tilde=0\n",
    "beta_tilde=0\n",
    "loss = []\n",
    "num_times=0\n",
    "loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_and_beta():\n",
    "    global alpha, beta,v_alpha,v_beta\n",
    "    #interim update\n",
    "    alpha_tilde = alpha + mom*v_alpha\n",
    "    beta_tilde = beta + mom*v_beta\n",
    "    \n",
    "    y_predicted = linear_function(alpha_tilde,beta_tilde,x)\n",
    "    derivative_of_loss_wrt_alpha_theta_tilde = 2*(y_predicted-y)\n",
    "    derivative_of_loss_wrt_beta_theta_tilde = x * derivative_of_loss_wrt_alpha_theta_tilde\n",
    "    #gradient update\n",
    "    v_alpha = mom*v_alpha - learning_rate*derivative_of_loss_wrt_alpha_theta_tilde.mean()\n",
    "    v_beta = mom*v_beta - learning_rate*derivative_of_loss_wrt_beta_theta_tilde.mean()\n",
    "    #velocity update\n",
    "    alpha = alpha + v_alpha\n",
    "    beta = beta + v_beta\n",
    "    l = (linear_function(alpha,beta,x)-y)**2\n",
    "    loss.append(l)\n",
    "def animate(i):\n",
    "    global num_times\n",
    "    num_times +=1\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(250):\n",
    "    update_alpha_and_beta()\n",
    "    print(\"alpha:\",alpha,\" beta:\",beta)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loss))\n",
    "x_axis=np.arange(0,250)\n",
    "plt.plot(x_axis,loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,250),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('Nesterov_animation.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Ada Optimization</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import random, randn\n",
    "from matplotlib import pyplot as plt, animation, rcParams, rc\n",
    "\n",
    "\n",
    "def linear_function(alpha,beta,x): return alpha+beta*x\n",
    "rc('animation',html='html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "beta_true = 6.0\n",
    "alpha_true = 3.0\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "learning_rate = 0.04\n",
    "\n",
    "#epsilon\n",
    "epsilon=0.9\n",
    "#r\n",
    "r_alpha = 0\n",
    "r_beta = 0\n",
    "delta = 10**(-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_and_beta():\n",
    "    global alpha, beta\n",
    "    r_alpha=0\n",
    "    r_beta=0 \n",
    "    y_predicted = linear_function(alpha,beta,x)\n",
    "    #gradient\n",
    "    g_alpha  = 2*(y_predicted-y)\n",
    "    g_beta = x * g_alpha\n",
    "    #accumulate squared gradient\n",
    "    r_alpha = r_alpha + np.multiply(g_alpha.mean(),g_alpha.mean())\n",
    "    r_beta = r_beta + np.multiply(g_beta.mean(),g_beta.mean())\n",
    "    #print(type(r_alpha))\n",
    "    #print(type(g_alpha))\n",
    "    print(g_alpha.mean())\n",
    "    delta_alpha =  -np.multiply(epsilon/(delta+math.sqrt(r_alpha),g_alpha.mean()))\n",
    "    delta_beta = -np.multiply(epsilon/(delta+math.sqrt(r_beta)),g_beta.mean())\n",
    "    #velocity update\n",
    "    alpha = alpha + delta_alpha.mean()\n",
    "    beta = beta + delta_beta.mean()\n",
    "    #print(\"alpha:\",alpha,\" beta:\",beta)\n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(300): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "update_alpha_and_beta()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,500),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('ada_animation.mp4', writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Adam</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import random, randn\n",
    "from matplotlib import pyplot as plt, animation, rcParams, rc\n",
    "\n",
    "\n",
    "def linear_function(alpha,beta,x): return alpha+beta*x\n",
    "rc('animation',html='html5')\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "beta_true = 6.0\n",
    "alpha_true = 3.0\n",
    "n = 30\n",
    "x = random(n)\n",
    "y = linear_function(alpha_true,beta_true,x)+0.2*randn(n)\n",
    "beta = -1.\n",
    "alpha = 3.\n",
    "learning_rate = 0.01\n",
    "\n",
    "#step size\n",
    "epsilon=0.001\n",
    "#p1,p1\n",
    "p1 = 0.9\n",
    "p2 = 0.999\n",
    "delta = 10**(-8)\n",
    "s_alpha=0\n",
    "s_beta=0\n",
    "r_alpha=0\n",
    "r_beta=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_and_beta():\n",
    "    global s_alpha, s_beta,r_alpha,r_beta, alpha,beta\n",
    "    for t in range(10):\n",
    "        for i in range(n):\n",
    "            y_predicted = linear_function(alpha,beta,x[i])\n",
    "            g_alpha  = (2/n)*(y_predicted-y[i])\n",
    "            g_beta = x[i] * g_alpha\n",
    "        #accumulate squared gradient\n",
    "        t+=1\n",
    "        s_alpha = p1*s_alpha + (1-p1)*g_alpha\n",
    "        s_beta = p1*s_beta + (1-p1)*g_beta\n",
    "        r_alpha = p2*r_alpha + (1-p2)*np.multiply(g_alpha,g_alpha)\n",
    "        r_beta = p2*r_beta + (1-p2)*np.multiply(g_beta,g_beta)\n",
    "        s_hat_alpha = s_alpha/(1-p1**t) \n",
    "        s_hat_beta =  s_beta/(1-p1**t)\n",
    "        r_hat_alpha = r_alpha/(1-p2**t)\n",
    "        r_hat_beta = r_beta/(1-p2**t)\n",
    "        delta_alpha = -epsilon*s_hat_alpha/(math.sqrt(r_hat_alpha)+delta) \n",
    "        delta_beta = -epsilon*s_hat_beta/(math.sqrt(r_hat_beta)+delta) \n",
    "        alpha += delta_alpha\n",
    "        beta += delta_beta\n",
    "    \n",
    "def animate(i):\n",
    "    x = np.linspace(0,1,100)\n",
    "    y = linear_function(alpha,beta,x)\n",
    "    line.set_data(x,y)\n",
    "    for i in range(20): \n",
    "        update_alpha_and_beta()\n",
    "    return (line,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=80,figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((-2,15))\n",
    "plt.scatter(x,y)\n",
    "line, = ax.plot([],[],lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation = animation.FuncAnimation(fig,animate,np.arange(0,250),interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulting_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "resulting_animation.save('adam_animation.mp4', writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
