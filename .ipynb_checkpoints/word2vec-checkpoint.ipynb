{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<img src=\"wv1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "text = \"natural language processing and machine learning is fun and exciting\"\n",
    "\n",
    "# Note the .lower() as upper and lowercase does not matter in our implementation\n",
    "# [['natural', 'language', 'processing', 'and', 'machine', 'learning', 'is', 'fun', 'and', 'exciting']]\n",
    "#corpus = list[]\n",
    "corpus = [[word.lower() for word in text.split()]]\n",
    "\n",
    "## Randomly initialise\n",
    "W1 = np.array([[0.236, -0.962, 0.686, 0.785, -0.454, -0.833, -0.744, 0.677, -0.427, -0.066],\n",
    "        [-0.907, 0.894, 0.225, 0.673, -0.579, -0.428, 0.685, 0.973, -0.070, -0.811],\n",
    "        [-0.576, 0.658, -0.582, -0.112, 0.662, 0.051, -0.401, -0.921, -0.158, 0.529],\n",
    "        [0.517, 0.436, 0.092, -0.835, -0.444, -0.905, 0.879, 0.303, 0.332, -0.275],\n",
    "        [0.859, -0.890, 0.651, 0.185, -0.511, -0.456, 0.377, -0.274, 0.182, -0.237],\n",
    "        [0.368, -0.867, -0.301, -0.222, 0.630, 0.808, 0.088, -0.902, -0.450, -0.408],\n",
    "        [0.728, 0.277, 0.439, 0.138, -0.943, -0.409, 0.687, -0.215, -0.807, 0.612],\n",
    "        [0.593, -0.699, 0.020, 0.142, -0.638, -0.633, 0.344, 0.868, 0.913, 0.429],\n",
    "        [0.447, -0.810, -0.061, -0.495, 0.794, -0.064, -0.817, -0.408, -0.286, 0.149]])\n",
    "\n",
    "W2 = np.array([[-0.868, -0.406, -0.288, -0.016, -0.560, 0.179, 0.099, 0.438, -0.551],\n",
    "        [-0.395, 0.890, 0.685, -0.329, 0.218, -0.852, -0.919, 0.665, 0.968],\n",
    "        [-0.128, 0.685, -0.828, 0.709, -0.420, 0.057, -0.212, 0.728, -0.690],\n",
    "        [0.881, 0.238, 0.018, 0.622, 0.936, -0.442, 0.936, 0.586, -0.020],\n",
    "        [-0.478, 0.240, 0.820, -0.731, 0.260, -0.989, -0.626, 0.796, -0.599],\n",
    "        [0.679, 0.721, -0.111, 0.083, -0.738, 0.227, 0.560, 0.929, 0.017],\n",
    "        [-0.690, 0.907, 0.464, -0.022, -0.005, -0.004, -0.425, 0.299, 0.757],\n",
    "        [-0.054, 0.397, -0.017, -0.563, -0.551, 0.465, -0.596, -0.413, -0.395],\n",
    "        [-0.838, 0.053, -0.160, -0.164, -0.671, 0.140, -0.149, 0.708, 0.425],\n",
    "        [0.096, -0.995, -0.313, 0.881, -0.402, -0.631, -0.660, 0.184, 0.487]])\n",
    "\n",
    "np.random.seed(43)\n",
    "#list of 9 arrays\n",
    "w1 = np.random.rand(90).reshape(10,9)\n",
    "w2 = np.random.rand(90).reshape(10,9)\n",
    "w1 = w1.tolist()\n",
    "w2 = w2.tolist() #list of 10 9 element 1d ndarray\n",
    "\n",
    "class word2vec:\n",
    "    def __init__(self):\n",
    "        self.n = 10 #dim of word embedding\n",
    "        self.lr = .01\n",
    "        self.epochs = 100 \n",
    "        self.window = 4 #+/- from center word\n",
    "        \n",
    "    def word2onehot(self, word):\n",
    "        print(\"word:\",word)\n",
    "        word_vec = np.zeros(self.v_count)\n",
    "    # Get ID of word from word_index\n",
    "        word_index = self.word_index[word]\n",
    "    # Change value from 0 to 1 according to ID of the word\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "        \n",
    "    def word_vec(self, word):\n",
    "        w_index = self.word_index[word]\n",
    "        v_w = self.w1[w_index]\n",
    "        return v_w\n",
    "    #convert word to integers\n",
    "    #convert corpus to dictionary[key=int representing word, value=freq]\n",
    "    def generate_training_data(self, corpus):\n",
    "        # Find unique word counts using dictonary\n",
    "        word_counts = defaultdict(int)\n",
    "        #corpus = list of list. \n",
    "        for row in corpus:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "        ## How many unique words in vocab? 9\n",
    "        self.v_count = len(word_counts.keys())\n",
    "        # Generate Lookup Dictionaries (vocab)\n",
    "        self.words_list = list(word_counts.keys())\n",
    "        print(\"created self.words_list\")\n",
    "        # Generate word:index\n",
    "        self.word_index = dict((word, i) for i, word in enumerate(self.words_list))\n",
    "        print(\"created self.word_index\")\n",
    "        # Generate index:word\n",
    "        self.index_word = dict((i, word) for i, word in enumerate(self.words_list))\n",
    "        training_data = []\n",
    "        # Cycle through each sentence in corpus\n",
    "        for sentence in corpus:\n",
    "            sent_len = len(sentence)\n",
    "              # Cycle through each word in sentence\n",
    "            for i, word in enumerate(sentence):\n",
    "                # Convert target word to one-hot\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                # Cycle through context window\n",
    "                w_context = []\n",
    "                # Note: window_size 2 will have range of 5 values\n",
    "                for j in range(i - self.window, i + self.window+1):\n",
    "                    # Criteria for context word \n",
    "                    # 1. Target word cannot be context word (j != i)\n",
    "                    # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "                    # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "                    if j != i and j <= sent_len-1 and j >= 0:\n",
    "                    # Append the one-hot representation of word to w_context\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "                    # print(sentence[i], sentence[j]) \n",
    "                    # training_data contains a one-hot representation of the target word and context words\n",
    "                training_data.append([w_target, w_context])\n",
    "        return np.array(training_data)\n",
    "\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        self.w1 = W1\n",
    "        self.w2 = W2\n",
    "        \n",
    "        # Cycle through each epoch\n",
    "        for i in range(self.epochs):\n",
    "            # Intialise loss to 0\n",
    "            self.loss = 0\n",
    "            # Cycle through each training sample\n",
    "            # w_t = vector for target word, w_c = vectors for context words\n",
    "            for w_t, w_c in training_data:\n",
    "                # Forward pass\n",
    "                # 1. predicted y using softmax (y_pred) 2. matrix of hidden layer (h) 3. output layer before softmax (u)\n",
    "                y_pred, h, u = self.forward_pass(w_t)\n",
    "                #########################################\n",
    "                # print(\"Vector for target word:\", w_t)\t#\n",
    "                # print(\"W1-before backprop\", self.w1)\t#\n",
    "                # print(\"W2-before backprop\", self.w2)\t#\n",
    "                #########################################\n",
    "\n",
    "                # Calculate error\n",
    "                # 1. For a target word, calculate difference between y_pred and each of the context words\n",
    "                # 2. Sum up the differences using np.sum to give us the error for this particular target word\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_c], axis=0)\n",
    "                #########################\n",
    "                # print(\"Error\", EI)\t#\n",
    "                #########################\n",
    "\n",
    "                # Backpropagation\n",
    "                # We use SGD to backpropagate errors - calculate loss on the output layer \n",
    "                self.backprop(EI, h, w_t)\n",
    "                #########################################\n",
    "                #print(\"W1-after backprop\", self.w1)\t#\n",
    "                #print(\"W2-after backprop\", self.w2)\t#\n",
    "                #########################################\n",
    "\n",
    "                # Calculate loss\n",
    "                # There are 2 parts to the loss function\n",
    "                # Part 1: -ve sum of all the output +\n",
    "                # Part 2: length of context words * log of sum for all elements (exponential-ed) in the output layer before softmax (u)\n",
    "                # Note: word.index(1) returns the index in the context word vector with value 1\n",
    "                # Note: u[word.index(1)] returns the value of the output layer before softmax\n",
    "                self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "                #############################################################\n",
    "                # Break if you want to see weights after first target word \t#\n",
    "                # break \t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "                #############################################################\n",
    "            print('Epoch:', i, \"Loss:\", self.loss)\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        # x is one-hot vector for target word, shape - 9x1\n",
    "        # Run through first matrix (w1) to get hidden layer - 10x9 dot 9x1 gives us 10x1\n",
    "        h = np.dot(x, self.w1)\n",
    "        # Dot product hidden layer with second matrix (w2) - 9x10 dot 10x1 gives us 9x1\n",
    "        u = np.dot(h, self.w2)\n",
    "        # Run 1x9 through softmax to force each element to range of [0, 1] - 1x8\n",
    "        y_c = self.softmax(u)\n",
    "        return y_c, h, u\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def backprop(self, e, h, x):\n",
    "        # https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.outer.html\n",
    "        # Column vector EI represents row-wise sum of prediction errors across each context word for the current center word\n",
    "        # Going backwards, we need to take derivative of E with respect of w2\n",
    "        # h - shape 10x1, e - shape 9x1, dl_dw2 - shape 10x9\n",
    "        # x - shape 9x1, w2 - 10x9, e.T - 9x1\n",
    "        dl_dw2 = np.outer(h, e)\n",
    "        dl_dw1 = np.outer(x, np.dot(self.w2, e.T))\n",
    "        ########################################\n",
    "        # print('Delta for w2', dl_dw2)\t\t\t#\n",
    "        # print('Hidden layer', h)\t\t\t\t#\n",
    "        # print('np.dot', np.dot(self.w2, e.T))\t#\n",
    "        # print('Delta for w1', dl_dw1)\t\t\t#\n",
    "        #########################################\n",
    "\n",
    "        # Update weights\n",
    "        self.w1 = self.w1 - (self.lr * dl_dw1)\n",
    "        self.w2 = self.w2 - (self.lr * dl_dw2)\n",
    "\n",
    "\n",
    "    # Input vector, returns nearest word(s)\n",
    "    def vec_sim(self, word, top_n):\n",
    "        v_w1 = self.word_vec(word)\n",
    "        word_sim = {}\n",
    "\n",
    "        for i in range(self.v_count):\n",
    "            # Find the similary score for each word in vocab\n",
    "            v_w2 = self.w1[i]\n",
    "            theta_sum = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_sum / theta_den\n",
    "            \n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print(word, sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created self.words_list\n",
      "created self.word_index\n",
      "word: natural\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: natural\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: natural\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: natural\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: natural\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: language\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: exciting\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: processing\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: exciting\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: exciting\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: machine\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: exciting\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: exciting\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: learning\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: is\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: fun\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "word: and\n",
      "word_vec: <bound method word2vec.word_vec of <__main__.word2vec object at 0x7f45814f8e48>>\n",
      "Epoch: 0 Loss: 152.70495654567281\n",
      "Epoch: 1 Loss: 147.92264536934664\n",
      "Epoch: 2 Loss: 144.14331613473703\n",
      "Epoch: 3 Loss: 141.09651521014413\n",
      "Epoch: 4 Loss: 138.59414142979887\n",
      "Epoch: 5 Loss: 136.50376007589503\n",
      "Epoch: 6 Loss: 134.73059774756607\n",
      "Epoch: 7 Loss: 133.20563813577348\n",
      "Epoch: 8 Loss: 131.8777611520022\n",
      "Epoch: 9 Loss: 130.70849885321806\n",
      "Epoch: 10 Loss: 129.66848185229182\n",
      "Epoch: 11 Loss: 128.73498886619848\n",
      "Epoch: 12 Loss: 127.89022750156096\n",
      "Epoch: 13 Loss: 127.12010838425779\n",
      "Epoch: 14 Loss: 126.41335805089749\n",
      "Epoch: 15 Loss: 125.76086835200033\n",
      "Epoch: 16 Loss: 125.1552135072205\n",
      "Epoch: 17 Loss: 124.59028764040589\n",
      "Epoch: 18 Loss: 124.06102996659486\n",
      "Epoch: 19 Loss: 123.563214457826\n",
      "Epoch: 20 Loss: 123.09328741961339\n",
      "Epoch: 21 Loss: 122.64824099604739\n",
      "Epoch: 22 Loss: 122.2255138486662\n",
      "Epoch: 23 Loss: 121.82291255257773\n",
      "Epoch: 24 Loss: 121.43854890782656\n",
      "Epoch: 25 Loss: 121.070789566626\n",
      "Epoch: 26 Loss: 120.71821525891828\n",
      "Epoch: 27 Loss: 120.37958755052169\n",
      "Epoch: 28 Loss: 120.05382155342794\n",
      "Epoch: 29 Loss: 119.73996337159335\n",
      "Epoch: 30 Loss: 119.43717133998122\n",
      "Epoch: 31 Loss: 119.14470032282235\n",
      "Epoch: 32 Loss: 118.86188849591694\n",
      "Epoch: 33 Loss: 118.58814615963063\n",
      "Epoch: 34 Loss: 118.32294622313971\n",
      "Epoch: 35 Loss: 118.0658160731973\n",
      "Epoch: 36 Loss: 117.8163305972745\n",
      "Epoch: 37 Loss: 117.57410617515643\n",
      "Epoch: 38 Loss: 117.33879548780982\n",
      "Epoch: 39 Loss: 117.11008301975684\n",
      "Epoch: 40 Loss: 116.88768115294276\n",
      "Epoch: 41 Loss: 116.67132676744723\n",
      "Epoch: 42 Loss: 116.460778278328\n",
      "Epoch: 43 Loss: 116.2558130491553\n",
      "Epoch: 44 Loss: 116.05622513197206\n",
      "Epoch: 45 Loss: 115.86182329094608\n",
      "Epoch: 46 Loss: 115.67242927320711\n",
      "Epoch: 47 Loss: 115.48787629555444\n",
      "Epoch: 48 Loss: 115.30800772007669\n",
      "Epoch: 49 Loss: 115.13267589541165\n",
      "Epoch: 50 Loss: 114.96174114350227\n",
      "Epoch: 51 Loss: 114.79507087437703\n",
      "Epoch: 52 Loss: 114.63253881376774\n",
      "Epoch: 53 Loss: 114.47402433033572\n",
      "Epoch: 54 Loss: 114.31941185095346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 Loss: 114.16859035392086\n",
      "Epoch: 56 Loss: 114.02145293121819\n",
      "Epoch: 57 Loss: 113.87789641193595\n",
      "Epoch: 58 Loss: 113.73782103990189\n",
      "Epoch: 59 Loss: 113.60113019927125\n",
      "Epoch: 60 Loss: 113.46773018247501\n",
      "Epoch: 61 Loss: 113.33752999545514\n",
      "Epoch: 62 Loss: 113.21044119557133\n",
      "Epoch: 63 Loss: 113.08637775795656\n",
      "Epoch: 64 Loss: 112.9652559664429\n",
      "Epoch: 65 Loss: 112.84699432548807\n",
      "Epoch: 66 Loss: 112.73151348981479\n",
      "Epoch: 67 Loss: 112.61873620874107\n",
      "Epoch: 68 Loss: 112.50858728243334\n",
      "Epoch: 69 Loss: 112.40099352756144\n",
      "Epoch: 70 Loss: 112.29588375007928\n",
      "Epoch: 71 Loss: 112.19318872309611\n",
      "Epoch: 72 Loss: 112.09284116804173\n",
      "Epoch: 73 Loss: 111.99477573756612\n",
      "Epoch: 74 Loss: 111.89892899884308\n",
      "Epoch: 75 Loss: 111.80523941617143\n",
      "Epoch: 76 Loss: 111.71364733198092\n",
      "Epoch: 77 Loss: 111.62409494555105\n",
      "Epoch: 78 Loss: 111.5365262889383\n",
      "Epoch: 79 Loss: 111.4508871997787\n",
      "Epoch: 80 Loss: 111.36712529078576\n",
      "Epoch: 81 Loss: 111.28518991589895\n",
      "Epoch: 82 Loss: 111.20503213315384\n",
      "Epoch: 83 Loss: 111.12660466444255\n",
      "Epoch: 84 Loss: 111.04986185241087\n",
      "Epoch: 85 Loss: 110.9747596148003\n",
      "Epoch: 86 Loss: 110.90125539658696\n",
      "Epoch: 87 Loss: 110.82930812029886\n",
      "Epoch: 88 Loss: 110.75887813490948\n",
      "Epoch: 89 Loss: 110.68992716370813\n",
      "Epoch: 90 Loss: 110.62241825154337\n",
      "Epoch: 91 Loss: 110.55631571181982\n",
      "Epoch: 92 Loss: 110.49158507360829\n",
      "Epoch: 93 Loss: 110.42819302920252\n",
      "Epoch: 94 Loss: 110.3661073824255\n",
      "Epoch: 95 Loss: 110.30529699795636\n",
      "Epoch: 96 Loss: 110.24573175191387\n",
      "Epoch: 97 Loss: 110.18738248389911\n",
      "Epoch: 98 Loss: 110.13022095066547\n",
      "Epoch: 99 Loss: 110.07421978155138\n",
      "Epoch: 100 Loss: 110.01935243577996\n",
      "Epoch: 101 Loss: 109.96559316170087\n",
      "Epoch: 102 Loss: 109.91291695802235\n",
      "Epoch: 103 Loss: 109.86129953705742\n",
      "Epoch: 104 Loss: 109.81071728998613\n",
      "Epoch: 105 Loss: 109.76114725411718\n",
      "Epoch: 106 Loss: 109.71256708211492\n",
      "Epoch: 107 Loss: 109.66495501314432\n",
      "Epoch: 108 Loss: 109.61828984587451\n",
      "Epoch: 109 Loss: 109.57255091327181\n",
      "Epoch: 110 Loss: 109.5277180591068\n",
      "Epoch: 111 Loss: 109.4837716160928\n",
      "Epoch: 112 Loss: 109.44069238557091\n",
      "Epoch: 113 Loss: 109.39846161865361\n",
      "Epoch: 114 Loss: 109.35706099873782\n",
      "Epoch: 115 Loss: 109.31647262529884\n",
      "Epoch: 116 Loss: 109.27667899887753\n",
      "Epoch: 117 Loss: 109.2376630071748\n",
      "Epoch: 118 Loss: 109.19940791217039\n",
      "Epoch: 119 Loss: 109.16189733818533\n",
      "Epoch: 120 Loss: 109.1251152608119\n",
      "Epoch: 121 Loss: 109.08904599663782\n",
      "Epoch: 122 Loss: 109.05367419369583\n",
      "Epoch: 123 Loss: 109.01898482257367\n",
      "Epoch: 124 Loss: 108.98496316812391\n",
      "Epoch: 125 Loss: 108.95159482171698\n",
      "Epoch: 126 Loss: 108.91886567398474\n",
      "Epoch: 127 Loss: 108.88676190800639\n",
      "Epoch: 128 Loss: 108.85526999289159\n",
      "Epoch: 129 Loss: 108.82437667772\n",
      "Epoch: 130 Loss: 108.79406898580008\n",
      "Epoch: 131 Loss: 108.76433420921227\n",
      "Epoch: 132 Loss: 108.7351599036062\n",
      "Epoch: 133 Loss: 108.70653388322368\n",
      "Epoch: 134 Loss: 108.67844421612162\n",
      "Epoch: 135 Loss: 108.6508792195731\n",
      "Epoch: 136 Loss: 108.62382745562525\n",
      "Epoch: 137 Loss: 108.59727772679611\n",
      "Epoch: 138 Loss: 108.5712190718942\n",
      "Epoch: 139 Loss: 108.5456407619463\n",
      "Epoch: 140 Loss: 108.52053229622065\n",
      "Epoch: 141 Loss: 108.49588339833457\n",
      "Epoch: 142 Loss: 108.47168401243634\n",
      "Epoch: 143 Loss: 108.44792429945306\n",
      "Epoch: 144 Loss: 108.42459463339674\n",
      "Epoch: 145 Loss: 108.40168559772256\n",
      "Epoch: 146 Loss: 108.37918798173338\n",
      "Epoch: 147 Loss: 108.35709277702624\n",
      "Epoch: 148 Loss: 108.33539117397665\n",
      "Epoch: 149 Loss: 108.31407455825745\n",
      "Epoch: 150 Loss: 108.2931345073898\n",
      "Epoch: 151 Loss: 108.27256278732376\n",
      "Epoch: 152 Loss: 108.25235134904716\n",
      "Epoch: 153 Loss: 108.2324923252214\n",
      "Epoch: 154 Loss: 108.21297802684309\n",
      "Epoch: 155 Loss: 108.19380093993135\n",
      "Epoch: 156 Loss: 108.17495372224006\n",
      "Epoch: 157 Loss: 108.15642919999542\n",
      "Epoch: 158 Loss: 108.13822036465862\n",
      "Epoch: 159 Loss: 108.12032036971415\n",
      "Epoch: 160 Loss: 108.10272252748445\n",
      "Epoch: 161 Loss: 108.08542030597089\n",
      "Epoch: 162 Loss: 108.06840732572263\n",
      "Epoch: 163 Loss: 108.05167735673342\n",
      "Epoch: 164 Loss: 108.03522431536761\n",
      "Epoch: 165 Loss: 108.01904226131637\n",
      "Epoch: 166 Loss: 108.00312539458469\n",
      "Epoch: 167 Loss: 107.9874680525103\n",
      "Epoch: 168 Loss: 107.97206470681581\n",
      "Epoch: 169 Loss: 107.95690996069435\n",
      "Epoch: 170 Loss: 107.94199854593022\n",
      "Epoch: 171 Loss: 107.92732532005543\n",
      "Epoch: 172 Loss: 107.91288526354268\n",
      "Epoch: 173 Loss: 107.89867347703576\n",
      "Epoch: 174 Loss: 107.88468517861871\n",
      "Epoch: 175 Loss: 107.87091570112358\n",
      "Epoch: 176 Loss: 107.85736048947827\n",
      "Epoch: 177 Loss: 107.84401509809479\n",
      "Epoch: 178 Loss: 107.8308751882986\n",
      "Epoch: 179 Loss: 107.81793652579948\n",
      "Epoch: 180 Loss: 107.80519497820458\n",
      "Epoch: 181 Loss: 107.79264651257408\n",
      "Epoch: 182 Loss: 107.78028719301953\n",
      "Epoch: 183 Loss: 107.76811317834553\n",
      "Epoch: 184 Loss: 107.75612071973495\n",
      "Epoch: 185 Loss: 107.74430615847736\n",
      "Epoch: 186 Loss: 107.73266592374166\n",
      "Epoch: 187 Loss: 107.72119653039233\n",
      "Epoch: 188 Loss: 107.70989457684921\n",
      "Epoch: 189 Loss: 107.69875674299136\n",
      "Epoch: 190 Loss: 107.68777978810459\n",
      "Epoch: 191 Loss: 107.67696054887215\n",
      "Epoch: 192 Loss: 107.66629593740902\n",
      "Epoch: 193 Loss: 107.65578293933902\n",
      "Epoch: 194 Loss: 107.64541861191495\n",
      "Epoch: 195 Loss: 107.63520008218077\n",
      "Epoch: 196 Loss: 107.62512454517608\n",
      "Epoch: 197 Loss: 107.61518926218216\n",
      "Epoch: 198 Loss: 107.60539155900922\n",
      "Epoch: 199 Loss: 107.59572882432434\n",
      "Epoch: 200 Loss: 107.58619850801952\n",
      "Epoch: 201 Loss: 107.57679811961961\n",
      "Epoch: 202 Loss: 107.56752522672919\n",
      "Epoch: 203 Loss: 107.55837745351803\n",
      "Epoch: 204 Loss: 107.54935247924462\n",
      "Epoch: 205 Loss: 107.54044803681678\n",
      "Epoch: 206 Loss: 107.53166191138922\n",
      "Epoch: 207 Loss: 107.52299193899695\n",
      "Epoch: 208 Loss: 107.51443600522423\n",
      "Epoch: 209 Loss: 107.50599204390828\n",
      "Epoch: 210 Loss: 107.49765803587677\n",
      "Epoch: 211 Loss: 107.48943200771919\n",
      "Epoch: 212 Loss: 107.48131203059023\n",
      "Epoch: 213 Loss: 107.47329621904595\n",
      "Epoch: 214 Loss: 107.46538272991053\n",
      "Epoch: 215 Loss: 107.45756976117427\n",
      "Epoch: 216 Loss: 107.44985555092097\n",
      "Epoch: 217 Loss: 107.44223837628508\n",
      "Epoch: 218 Loss: 107.43471655243701\n",
      "Epoch: 219 Loss: 107.42728843159655\n",
      "Epoch: 220 Loss: 107.4199524020735\n",
      "Epoch: 221 Loss: 107.41270688733475\n",
      "Epoch: 222 Loss: 107.40555034509723\n",
      "Epoch: 223 Loss: 107.39848126644627\n",
      "Epoch: 224 Loss: 107.39149817497834\n",
      "Epoch: 225 Loss: 107.38459962596792\n",
      "Epoch: 226 Loss: 107.37778420555753\n",
      "Epoch: 227 Loss: 107.37105052997053\n",
      "Epoch: 228 Loss: 107.36439724474613\n",
      "Epoch: 229 Loss: 107.35782302399558\n",
      "Epoch: 230 Loss: 107.35132656967973\n",
      "Epoch: 231 Loss: 107.34490661090656\n",
      "Epoch: 232 Loss: 107.33856190324857\n",
      "Epoch: 233 Loss: 107.33229122807941\n",
      "Epoch: 234 Loss: 107.32609339192943\n",
      "Epoch: 235 Loss: 107.31996722585873\n",
      "Epoch: 236 Loss: 107.31391158484871\n",
      "Epoch: 237 Loss: 107.3079253472101\n",
      "Epoch: 238 Loss: 107.30200741400768\n",
      "Epoch: 239 Loss: 107.29615670850164\n",
      "Epoch: 240 Loss: 107.29037217560396\n",
      "Epoch: 241 Loss: 107.28465278135047\n",
      "Epoch: 242 Loss: 107.27899751238765\n",
      "Epoch: 243 Loss: 107.27340537547343\n",
      "Epoch: 244 Loss: 107.26787539699241\n",
      "Epoch: 245 Loss: 107.26240662248405\n",
      "Epoch: 246 Loss: 107.25699811618452\n",
      "Epoch: 247 Loss: 107.25164896058078\n",
      "Epoch: 248 Loss: 107.24635825597726\n",
      "Epoch: 249 Loss: 107.24112512007447\n",
      "Epoch: 250 Loss: 107.23594868755914\n",
      "Epoch: 251 Loss: 107.23082810970575\n",
      "Epoch: 252 Loss: 107.22576255398873\n",
      "Epoch: 253 Loss: 107.22075120370569\n",
      "Epoch: 254 Loss: 107.21579325761041\n",
      "Epoch: 255 Loss: 107.21088792955616\n",
      "Epoch: 256 Loss: 107.20603444814857\n",
      "Epoch: 257 Loss: 107.20123205640797\n",
      "Epoch: 258 Loss: 107.19648001144041\n",
      "Epoch: 259 Loss: 107.19177758411833\n",
      "Epoch: 260 Loss: 107.18712405876877\n",
      "Epoch: 261 Loss: 107.18251873287075\n",
      "Epoch: 262 Loss: 107.1779609167602\n",
      "Epoch: 263 Loss: 107.17344993334285\n",
      "Epoch: 264 Loss: 107.16898511781463\n",
      "Epoch: 265 Loss: 107.16456581738962\n",
      "Epoch: 266 Loss: 107.16019139103491\n",
      "Epoch: 267 Loss: 107.1558612092124\n",
      "Epoch: 268 Loss: 107.15157465362745\n",
      "Epoch: 269 Loss: 107.14733111698393\n",
      "Epoch: 270 Loss: 107.14313000274562\n",
      "Epoch: 271 Loss: 107.13897072490376\n",
      "Epoch: 272 Loss: 107.13485270775054\n",
      "Epoch: 273 Loss: 107.1307753856584\n",
      "Epoch: 274 Loss: 107.12673820286476\n",
      "Epoch: 275 Loss: 107.12274061326261\n",
      "Epoch: 276 Loss: 107.11878208019588\n",
      "Epoch: 277 Loss: 107.11486207626031\n",
      "Epoch: 278 Loss: 107.11098008310913\n",
      "Epoch: 279 Loss: 107.1071355912637\n",
      "Epoch: 280 Loss: 107.10332809992866\n",
      "Epoch: 281 Loss: 107.0995571168119\n",
      "Epoch: 282 Loss: 107.09582215794873\n",
      "Epoch: 283 Loss: 107.09212274753055\n",
      "Epoch: 284 Loss: 107.08845841773748\n",
      "Epoch: 285 Loss: 107.08482870857537\n",
      "Epoch: 286 Loss: 107.08123316771649\n",
      "Epoch: 287 Loss: 107.0776713503441\n",
      "Epoch: 288 Loss: 107.07414281900101\n",
      "Epoch: 289 Loss: 107.0706471434413\n",
      "Epoch: 290 Loss: 107.06718390048611\n",
      "Epoch: 291 Loss: 107.06375267388246\n",
      "Epoch: 292 Loss: 107.06035305416549\n",
      "Epoch: 293 Loss: 107.05698463852406\n",
      "Epoch: 294 Loss: 107.0536470306695\n",
      "Epoch: 295 Loss: 107.0503398407072\n",
      "Epoch: 296 Loss: 107.0470626850115\n",
      "Epoch: 297 Loss: 107.04381518610333\n",
      "Epoch: 298 Loss: 107.04059697253057\n",
      "Epoch: 299 Loss: 107.03740767875138\n",
      "Epoch: 300 Loss: 107.03424694502003\n",
      "Epoch: 301 Loss: 107.03111441727543\n",
      "Epoch: 302 Loss: 107.02800974703229\n",
      "Epoch: 303 Loss: 107.02493259127431\n",
      "Epoch: 304 Loss: 107.02188261235048\n",
      "Epoch: 305 Loss: 107.01885947787304\n",
      "Epoch: 306 Loss: 107.01586286061817\n",
      "Epoch: 307 Loss: 107.01289243842871\n",
      "Epoch: 308 Loss: 107.009947894119\n",
      "Epoch: 309 Loss: 107.00702891538201\n",
      "Epoch: 310 Loss: 107.00413519469828\n",
      "Epoch: 311 Loss: 107.00126642924718\n",
      "Epoch: 312 Loss: 106.99842232081974\n",
      "Epoch: 313 Loss: 106.99560257573368\n",
      "Epoch: 314 Loss: 106.99280690475014\n",
      "Epoch: 315 Loss: 106.99003502299222\n",
      "Epoch: 316 Loss: 106.98728664986541\n",
      "Epoch: 317 Loss: 106.98456150897954\n",
      "Epoch: 318 Loss: 106.98185932807252\n",
      "Epoch: 319 Loss: 106.97917983893574\n",
      "Epoch: 320 Loss: 106.97652277734086\n",
      "Epoch: 321 Loss: 106.97388788296851\n",
      "Epoch: 322 Loss: 106.97127489933813\n",
      "Epoch: 323 Loss: 106.96868357373948\n",
      "Epoch: 324 Loss: 106.96611365716552\n",
      "Epoch: 325 Loss: 106.96356490424687\n",
      "Epoch: 326 Loss: 106.96103707318726\n",
      "Epoch: 327 Loss: 106.95852992570077\n",
      "Epoch: 328 Loss: 106.95604322695004\n",
      "Epoch: 329 Loss: 106.95357674548589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330 Loss: 106.95113025318825\n",
      "Epoch: 331 Loss: 106.94870352520802\n",
      "Epoch: 332 Loss: 106.94629633991053\n",
      "Epoch: 333 Loss: 106.9439084788198\n",
      "Epoch: 334 Loss: 106.9415397265641\n",
      "Epoch: 335 Loss: 106.93918987082267\n",
      "Epoch: 336 Loss: 106.93685870227326\n",
      "Epoch: 337 Loss: 106.93454601454106\n",
      "Epoch: 338 Loss: 106.93225160414836\n",
      "Epoch: 339 Loss: 106.92997527046543\n",
      "Epoch: 340 Loss: 106.92771681566222\n",
      "Epoch: 341 Loss: 106.92547604466118\n",
      "Epoch: 342 Loss: 106.92325276509087\n",
      "Epoch: 343 Loss: 106.92104678724051\n",
      "Epoch: 344 Loss: 106.91885792401555\n",
      "Epoch: 345 Loss: 106.91668599089392\n",
      "Epoch: 346 Loss: 106.91453080588335\n",
      "Epoch: 347 Loss: 106.91239218947929\n",
      "Epoch: 348 Loss: 106.91026996462384\n",
      "Epoch: 349 Loss: 106.90816395666539\n",
      "Epoch: 350 Loss: 106.90607399331904\n",
      "Epoch: 351 Loss: 106.90399990462784\n",
      "Epoch: 352 Loss: 106.90194152292466\n",
      "Epoch: 353 Loss: 106.89989868279487\n",
      "Epoch: 354 Loss: 106.89787122103982\n",
      "Epoch: 355 Loss: 106.89585897664087\n",
      "Epoch: 356 Loss: 106.89386179072405\n",
      "Epoch: 357 Loss: 106.89187950652565\n",
      "Epoch: 358 Loss: 106.88991196935825\n",
      "Epoch: 359 Loss: 106.88795902657739\n",
      "Epoch: 360 Loss: 106.886020527549\n",
      "Epoch: 361 Loss: 106.88409632361727\n",
      "Epoch: 362 Loss: 106.88218626807335\n",
      "Epoch: 363 Loss: 106.88029021612432\n",
      "Epoch: 364 Loss: 106.87840802486302\n",
      "Epoch: 365 Loss: 106.87653955323832\n",
      "Epoch: 366 Loss: 106.87468466202591\n",
      "Epoch: 367 Loss: 106.8728432137996\n",
      "Epoch: 368 Loss: 106.87101507290349\n",
      "Epoch: 369 Loss: 106.86920010542396\n",
      "Epoch: 370 Loss: 106.8673981791629\n",
      "Epoch: 371 Loss: 106.86560916361101\n",
      "Epoch: 372 Loss: 106.86383292992161\n",
      "Epoch: 373 Loss: 106.86206935088511\n",
      "Epoch: 374 Loss: 106.86031830090384\n",
      "Epoch: 375 Loss: 106.85857965596712\n",
      "Epoch: 376 Loss: 106.85685329362732\n",
      "Epoch: 377 Loss: 106.85513909297565\n",
      "Epoch: 378 Loss: 106.85343693461904\n",
      "Epoch: 379 Loss: 106.85174670065695\n",
      "Epoch: 380 Loss: 106.85006827465887\n",
      "Epoch: 381 Loss: 106.84840154164202\n",
      "Epoch: 382 Loss: 106.84674638804978\n",
      "Epoch: 383 Loss: 106.84510270172996\n",
      "Epoch: 384 Loss: 106.84347037191401\n",
      "Epoch: 385 Loss: 106.84184928919623\n",
      "Epoch: 386 Loss: 106.84023934551351\n",
      "Epoch: 387 Loss: 106.83864043412535\n",
      "Epoch: 388 Loss: 106.83705244959427\n",
      "Epoch: 389 Loss: 106.83547528776653\n",
      "Epoch: 390 Loss: 106.83390884575316\n",
      "Epoch: 391 Loss: 106.8323530219115\n",
      "Epoch: 392 Loss: 106.83080771582674\n",
      "Epoch: 393 Loss: 106.82927282829401\n",
      "Epoch: 394 Loss: 106.8277482613008\n",
      "Epoch: 395 Loss: 106.82623391800952\n",
      "Epoch: 396 Loss: 106.82472970274036\n",
      "Epoch: 397 Loss: 106.82323552095477\n",
      "Epoch: 398 Loss: 106.82175127923858\n",
      "Epoch: 399 Loss: 106.82027688528626\n",
      "Epoch: 400 Loss: 106.81881224788457\n",
      "Epoch: 401 Loss: 106.81735727689713\n",
      "Epoch: 402 Loss: 106.81591188324892\n",
      "Epoch: 403 Loss: 106.81447597891119\n",
      "Epoch: 404 Loss: 106.81304947688649\n",
      "Epoch: 405 Loss: 106.81163229119417\n",
      "Epoch: 406 Loss: 106.8102243368558\n",
      "Epoch: 407 Loss: 106.80882552988112\n",
      "Epoch: 408 Loss: 106.80743578725408\n",
      "Epoch: 409 Loss: 106.80605502691914\n",
      "Epoch: 410 Loss: 106.80468316776783\n",
      "Epoch: 411 Loss: 106.80332012962546\n",
      "Epoch: 412 Loss: 106.80196583323811\n",
      "Epoch: 413 Loss: 106.80062020025979\n",
      "Epoch: 414 Loss: 106.79928315323987\n",
      "Epoch: 415 Loss: 106.79795461561059\n",
      "Epoch: 416 Loss: 106.79663451167505\n",
      "Epoch: 417 Loss: 106.7953227665949\n",
      "Epoch: 418 Loss: 106.79401930637874\n",
      "Epoch: 419 Loss: 106.79272405787059\n",
      "Epoch: 420 Loss: 106.79143694873815\n",
      "Epoch: 421 Loss: 106.79015790746175\n",
      "Epoch: 422 Loss: 106.78888686332341\n",
      "Epoch: 423 Loss: 106.78762374639554\n",
      "Epoch: 424 Loss: 106.78636848753067\n",
      "Epoch: 425 Loss: 106.78512101835054\n",
      "Epoch: 426 Loss: 106.78388127123598\n",
      "Epoch: 427 Loss: 106.78264917931652\n",
      "Epoch: 428 Loss: 106.78142467646046\n",
      "Epoch: 429 Loss: 106.78020769726483\n",
      "Epoch: 430 Loss: 106.77899817704578\n",
      "Epoch: 431 Loss: 106.77779605182889\n",
      "Epoch: 432 Loss: 106.77660125833982\n",
      "Epoch: 433 Loss: 106.77541373399492\n",
      "Epoch: 434 Loss: 106.77423341689217\n",
      "Epoch: 435 Loss: 106.77306024580214\n",
      "Epoch: 436 Loss: 106.77189416015909\n",
      "Epoch: 437 Loss: 106.77073510005228\n",
      "Epoch: 438 Loss: 106.76958300621737\n",
      "Epoch: 439 Loss: 106.76843782002793\n",
      "Epoch: 440 Loss: 106.76729948348714\n",
      "Epoch: 441 Loss: 106.76616793921963\n",
      "Epoch: 442 Loss: 106.7650431304632\n",
      "Epoch: 443 Loss: 106.76392500106115\n",
      "Epoch: 444 Loss: 106.76281349545428\n",
      "Epoch: 445 Loss: 106.7617085586731\n",
      "Epoch: 446 Loss: 106.76061013633044\n",
      "Epoch: 447 Loss: 106.75951817461379\n",
      "Epoch: 448 Loss: 106.75843262027796\n",
      "Epoch: 449 Loss: 106.75735342063794\n",
      "Epoch: 450 Loss: 106.7562805235616\n",
      "Epoch: 451 Loss: 106.75521387746271\n",
      "Epoch: 452 Loss: 106.75415343129397\n",
      "Epoch: 453 Loss: 106.75309913454029\n",
      "Epoch: 454 Loss: 106.75205093721182\n",
      "Epoch: 455 Loss: 106.75100878983758\n",
      "Epoch: 456 Loss: 106.74997264345873\n",
      "Epoch: 457 Loss: 106.74894244962232\n",
      "Epoch: 458 Loss: 106.74791816037467\n",
      "Epoch: 459 Loss: 106.74689972825539\n",
      "Epoch: 460 Loss: 106.74588710629105\n",
      "Epoch: 461 Loss: 106.74488024798924\n",
      "Epoch: 462 Loss: 106.74387910733252\n",
      "Epoch: 463 Loss: 106.74288363877244\n",
      "Epoch: 464 Loss: 106.74189379722388\n",
      "Epoch: 465 Loss: 106.74090953805936\n",
      "Epoch: 466 Loss: 106.73993081710314\n",
      "Epoch: 467 Loss: 106.73895759062597\n",
      "Epoch: 468 Loss: 106.73798981533942\n",
      "Epoch: 469 Loss: 106.73702744839058\n",
      "Epoch: 470 Loss: 106.73607044735665\n",
      "Epoch: 471 Loss: 106.73511877023975\n",
      "Epoch: 472 Loss: 106.73417237546178\n",
      "Epoch: 473 Loss: 106.73323122185924\n",
      "Epoch: 474 Loss: 106.73229526867831\n",
      "Epoch: 475 Loss: 106.73136447556978\n",
      "Epoch: 476 Loss: 106.73043880258432\n",
      "Epoch: 477 Loss: 106.72951821016748\n",
      "Epoch: 478 Loss: 106.72860265915513\n",
      "Epoch: 479 Loss: 106.72769211076863\n",
      "Epoch: 480 Loss: 106.72678652661034\n",
      "Epoch: 481 Loss: 106.72588586865893\n",
      "Epoch: 482 Loss: 106.72499009926517\n",
      "Epoch: 483 Loss: 106.72409918114714\n",
      "Epoch: 484 Loss: 106.72321307738612\n",
      "Epoch: 485 Loss: 106.72233175142232\n",
      "Epoch: 486 Loss: 106.7214551670504\n",
      "Epoch: 487 Loss: 106.72058328841555\n",
      "Epoch: 488 Loss: 106.71971608000925\n",
      "Epoch: 489 Loss: 106.71885350666514\n",
      "Epoch: 490 Loss: 106.71799553355524\n",
      "Epoch: 491 Loss: 106.7171421261857\n",
      "Epoch: 492 Loss: 106.7162932503931\n",
      "Epoch: 493 Loss: 106.71544887234056\n",
      "Epoch: 494 Loss: 106.71460895851396\n",
      "Epoch: 495 Loss: 106.71377347571813\n",
      "Epoch: 496 Loss: 106.71294239107324\n",
      "Epoch: 497 Loss: 106.7121156720111\n",
      "Epoch: 498 Loss: 106.71129328627165\n",
      "Epoch: 499 Loss: 106.7104752018993\n",
      "Epoch: 500 Loss: 106.70966138723949\n",
      "Epoch: 501 Loss: 106.70885181093539\n",
      "Epoch: 502 Loss: 106.70804644192417\n",
      "Epoch: 503 Loss: 106.70724524943401\n",
      "Epoch: 504 Loss: 106.70644820298045\n",
      "Epoch: 505 Loss: 106.70565527236343\n",
      "Epoch: 506 Loss: 106.70486642766392\n",
      "Epoch: 507 Loss: 106.70408163924063\n",
      "Epoch: 508 Loss: 106.7033008777271\n",
      "Epoch: 509 Loss: 106.70252411402848\n",
      "Epoch: 510 Loss: 106.70175131931846\n",
      "Epoch: 511 Loss: 106.70098246503632\n",
      "Epoch: 512 Loss: 106.70021752288392\n",
      "Epoch: 513 Loss: 106.69945646482277\n",
      "Epoch: 514 Loss: 106.6986992630711\n",
      "Epoch: 515 Loss: 106.69794589010108\n",
      "Epoch: 516 Loss: 106.69719631863595\n",
      "Epoch: 517 Loss: 106.69645052164715\n",
      "Epoch: 518 Loss: 106.69570847235181\n",
      "Epoch: 519 Loss: 106.69497014420976\n",
      "Epoch: 520 Loss: 106.69423551092106\n",
      "Epoch: 521 Loss: 106.69350454642323\n",
      "Epoch: 522 Loss: 106.6927772248887\n",
      "Epoch: 523 Loss: 106.69205352072227\n",
      "Epoch: 524 Loss: 106.69133340855848\n",
      "Epoch: 525 Loss: 106.69061686325912\n",
      "Epoch: 526 Loss: 106.68990385991086\n",
      "Epoch: 527 Loss: 106.68919437382264\n",
      "Epoch: 528 Loss: 106.68848838052344\n",
      "Epoch: 529 Loss: 106.68778585575969\n",
      "Epoch: 530 Loss: 106.68708677549309\n",
      "Epoch: 531 Loss: 106.6863911158982\n",
      "Epoch: 532 Loss: 106.68569885336018\n",
      "Epoch: 533 Loss: 106.68500996447256\n",
      "Epoch: 534 Loss: 106.68432442603485\n",
      "Epoch: 535 Loss: 106.68364221505055\n",
      "Epoch: 536 Loss: 106.68296330872488\n",
      "Epoch: 537 Loss: 106.68228768446245\n",
      "Epoch: 538 Loss: 106.68161531986553\n",
      "Epoch: 539 Loss: 106.6809461927315\n",
      "Epoch: 540 Loss: 106.68028028105115\n",
      "Epoch: 541 Loss: 106.67961756300643\n",
      "Epoch: 542 Loss: 106.6789580169685\n",
      "Epoch: 543 Loss: 106.67830162149563\n",
      "Epoch: 544 Loss: 106.67764835533147\n",
      "Epoch: 545 Loss: 106.67699819740281\n",
      "Epoch: 546 Loss: 106.67635112681783\n",
      "Epoch: 547 Loss: 106.67570712286424\n",
      "Epoch: 548 Loss: 106.67506616500725\n",
      "Epoch: 549 Loss: 106.67442823288778\n",
      "Epoch: 550 Loss: 106.67379330632077\n",
      "Epoch: 551 Loss: 106.67316136529317\n",
      "Epoch: 552 Loss: 106.6725323899622\n",
      "Epoch: 553 Loss: 106.67190636065371\n",
      "Epoch: 554 Loss: 106.67128325786031\n",
      "Epoch: 555 Loss: 106.67066306223967\n",
      "Epoch: 556 Loss: 106.67004575461289\n",
      "Epoch: 557 Loss: 106.66943131596281\n",
      "Epoch: 558 Loss: 106.66881972743215\n",
      "Epoch: 559 Loss: 106.66821097032225\n",
      "Epoch: 560 Loss: 106.66760502609107\n",
      "Epoch: 561 Loss: 106.66700187635183\n",
      "Epoch: 562 Loss: 106.66640150287135\n",
      "Epoch: 563 Loss: 106.66580388756852\n",
      "Epoch: 564 Loss: 106.66520901251266\n",
      "Epoch: 565 Loss: 106.66461685992216\n",
      "Epoch: 566 Loss: 106.6640274121628\n",
      "Epoch: 567 Loss: 106.66344065174638\n",
      "Epoch: 568 Loss: 106.66285656132926\n",
      "Epoch: 569 Loss: 106.66227512371078\n",
      "Epoch: 570 Loss: 106.66169632183193\n",
      "Epoch: 571 Loss: 106.66112013877392\n",
      "Epoch: 572 Loss: 106.66054655775676\n",
      "Epoch: 573 Loss: 106.65997556213789\n",
      "Epoch: 574 Loss: 106.65940713541079\n",
      "Epoch: 575 Loss: 106.65884126120359\n",
      "Epoch: 576 Loss: 106.65827792327781\n",
      "Epoch: 577 Loss: 106.65771710552704\n",
      "Epoch: 578 Loss: 106.65715879197553\n",
      "Epoch: 579 Loss: 106.65660296677709\n",
      "Epoch: 580 Loss: 106.6560496142135\n",
      "Epoch: 581 Loss: 106.65549871869361\n",
      "Epoch: 582 Loss: 106.6549502647518\n",
      "Epoch: 583 Loss: 106.654404237047\n",
      "Epoch: 584 Loss: 106.6538606203612\n",
      "Epoch: 585 Loss: 106.6533193995985\n",
      "Epoch: 586 Loss: 106.65278055978371\n",
      "Epoch: 587 Loss: 106.65224408606136\n",
      "Epoch: 588 Loss: 106.65170996369442\n",
      "Epoch: 589 Loss: 106.65117817806313\n",
      "Epoch: 590 Loss: 106.65064871466396\n",
      "Epoch: 591 Loss: 106.65012155910846\n",
      "Epoch: 592 Loss: 106.64959669712202\n",
      "Epoch: 593 Loss: 106.64907411454303\n",
      "Epoch: 594 Loss: 106.64855379732151\n",
      "Epoch: 595 Loss: 106.6480357315182\n",
      "Epoch: 596 Loss: 106.64751990330353\n",
      "Epoch: 597 Loss: 106.64700629895644\n",
      "Epoch: 598 Loss: 106.64649490486345\n",
      "Epoch: 599 Loss: 106.64598570751753\n",
      "Epoch: 600 Loss: 106.64547869351723\n",
      "Epoch: 601 Loss: 106.64497384956557\n",
      "Epoch: 602 Loss: 106.64447116246903\n",
      "Epoch: 603 Loss: 106.64397061913665\n",
      "Epoch: 604 Loss: 106.64347220657909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 605 Loss: 106.64297591190744\n",
      "Epoch: 606 Loss: 106.6424817223326\n",
      "Epoch: 607 Loss: 106.64198962516407\n",
      "Epoch: 608 Loss: 106.64149960780922\n",
      "Epoch: 609 Loss: 106.64101165777217\n",
      "Epoch: 610 Loss: 106.64052576265313\n",
      "Epoch: 611 Loss: 106.64004191014727\n",
      "Epoch: 612 Loss: 106.63956008804398\n",
      "Epoch: 613 Loss: 106.6390802842259\n",
      "Epoch: 614 Loss: 106.63860248666819\n",
      "Epoch: 615 Loss: 106.63812668343749\n",
      "Epoch: 616 Loss: 106.63765286269116\n",
      "Epoch: 617 Loss: 106.6371810126765\n",
      "Epoch: 618 Loss: 106.63671112172983\n",
      "Epoch: 619 Loss: 106.63624317827569\n",
      "Epoch: 620 Loss: 106.63577717082605\n",
      "Epoch: 621 Loss: 106.63531308797951\n",
      "Epoch: 622 Loss: 106.63485091842043\n",
      "Epoch: 623 Loss: 106.63439065091825\n",
      "Epoch: 624 Loss: 106.63393227432664\n",
      "Epoch: 625 Loss: 106.63347577758279\n",
      "Epoch: 626 Loss: 106.63302114970656\n",
      "Epoch: 627 Loss: 106.63256837979982\n",
      "Epoch: 628 Loss: 106.63211745704565\n",
      "Epoch: 629 Loss: 106.63166837070764\n",
      "Epoch: 630 Loss: 106.63122111012905\n",
      "Epoch: 631 Loss: 106.6307756647323\n",
      "Epoch: 632 Loss: 106.63033202401806\n",
      "Epoch: 633 Loss: 106.62989017756462\n",
      "Epoch: 634 Loss: 106.62945011502721\n",
      "Epoch: 635 Loss: 106.62901182613729\n",
      "Epoch: 636 Loss: 106.62857530070184\n",
      "Epoch: 637 Loss: 106.6281405286027\n",
      "Epoch: 638 Loss: 106.62770749979592\n",
      "Epoch: 639 Loss: 106.62727620431112\n",
      "Epoch: 640 Loss: 106.62684663225076\n",
      "Epoch: 641 Loss: 106.62641877378954\n",
      "Epoch: 642 Loss: 106.62599261917372\n",
      "Epoch: 643 Loss: 106.62556815872053\n",
      "Epoch: 644 Loss: 106.62514538281758\n",
      "Epoch: 645 Loss: 106.62472428192213\n",
      "Epoch: 646 Loss: 106.62430484656052\n",
      "Epoch: 647 Loss: 106.62388706732757\n",
      "Epoch: 648 Loss: 106.623470934886\n",
      "Epoch: 649 Loss: 106.62305643996581\n",
      "Epoch: 650 Loss: 106.62264357336365\n",
      "Epoch: 651 Loss: 106.62223232594236\n",
      "Epoch: 652 Loss: 106.62182268863016\n",
      "Epoch: 653 Loss: 106.62141465242043\n",
      "Epoch: 654 Loss: 106.6210082083707\n",
      "Epoch: 655 Loss: 106.6206033476025\n",
      "Epoch: 656 Loss: 106.6202000613006\n",
      "Epoch: 657 Loss: 106.61979834071245\n",
      "Epoch: 658 Loss: 106.61939817714772\n",
      "Epoch: 659 Loss: 106.6189995619777\n",
      "Epoch: 660 Loss: 106.61860248663481\n",
      "Epoch: 661 Loss: 106.61820694261205\n",
      "Epoch: 662 Loss: 106.6178129214624\n",
      "Epoch: 663 Loss: 106.61742041479857\n",
      "Epoch: 664 Loss: 106.61702941429212\n",
      "Epoch: 665 Loss: 106.61663991167322\n",
      "Epoch: 666 Loss: 106.61625189873004\n",
      "Epoch: 667 Loss: 106.61586536730834\n",
      "Epoch: 668 Loss: 106.61548030931084\n",
      "Epoch: 669 Loss: 106.61509671669688\n",
      "Epoch: 670 Loss: 106.61471458148183\n",
      "Epoch: 671 Loss: 106.6143338957367\n",
      "Epoch: 672 Loss: 106.61395465158752\n",
      "Epoch: 673 Loss: 106.61357684121514\n",
      "Epoch: 674 Loss: 106.61320045685447\n",
      "Epoch: 675 Loss: 106.61282549079428\n",
      "Epoch: 676 Loss: 106.6124519353765\n",
      "Epoch: 677 Loss: 106.61207978299598\n",
      "Epoch: 678 Loss: 106.61170902609993\n",
      "Epoch: 679 Loss: 106.61133965718754\n",
      "Epoch: 680 Loss: 106.61097166880954\n",
      "Epoch: 681 Loss: 106.61060505356762\n",
      "Epoch: 682 Loss: 106.61023980411436\n",
      "Epoch: 683 Loss: 106.60987591315232\n",
      "Epoch: 684 Loss: 106.60951337343408\n",
      "Epoch: 685 Loss: 106.60915217776159\n",
      "Epoch: 686 Loss: 106.60879231898566\n",
      "Epoch: 687 Loss: 106.60843379000588\n",
      "Epoch: 688 Loss: 106.60807658376989\n",
      "Epoch: 689 Loss: 106.60772069327318\n",
      "Epoch: 690 Loss: 106.60736611155863\n",
      "Epoch: 691 Loss: 106.60701283171615\n",
      "Epoch: 692 Loss: 106.60666084688218\n",
      "Epoch: 693 Loss: 106.60631015023954\n",
      "Epoch: 694 Loss: 106.60596073501672\n",
      "Epoch: 695 Loss: 106.60561259448784\n",
      "Epoch: 696 Loss: 106.6052657219721\n",
      "Epoch: 697 Loss: 106.60492011083339\n",
      "Epoch: 698 Loss: 106.60457575448002\n",
      "Epoch: 699 Loss: 106.60423264636432\n",
      "Epoch: 700 Loss: 106.60389077998218\n",
      "Epoch: 701 Loss: 106.60355014887297\n",
      "Epoch: 702 Loss: 106.6032107466188\n",
      "Epoch: 703 Loss: 106.60287256684457\n",
      "Epoch: 704 Loss: 106.60253560321729\n",
      "Epoch: 705 Loss: 106.60219984944602\n",
      "Epoch: 706 Loss: 106.60186529928129\n",
      "Epoch: 707 Loss: 106.60153194651494\n",
      "Epoch: 708 Loss: 106.60119978497966\n",
      "Epoch: 709 Loss: 106.6008688085488\n",
      "Epoch: 710 Loss: 106.60053901113596\n",
      "Epoch: 711 Loss: 106.60021038669457\n",
      "Epoch: 712 Loss: 106.59988292921783\n",
      "Epoch: 713 Loss: 106.59955663273811\n",
      "Epoch: 714 Loss: 106.59923149132692\n",
      "Epoch: 715 Loss: 106.59890749909427\n",
      "Epoch: 716 Loss: 106.59858465018868\n",
      "Epoch: 717 Loss: 106.59826293879665\n",
      "Epoch: 718 Loss: 106.59794235914251\n",
      "Epoch: 719 Loss: 106.59762290548801\n",
      "Epoch: 720 Loss: 106.59730457213209\n",
      "Epoch: 721 Loss: 106.59698735341055\n",
      "Epoch: 722 Loss: 106.59667124369585\n",
      "Epoch: 723 Loss: 106.59635623739663\n",
      "Epoch: 724 Loss: 106.59604232895761\n",
      "Epoch: 725 Loss: 106.59572951285926\n",
      "Epoch: 726 Loss: 106.59541778361745\n",
      "Epoch: 727 Loss: 106.59510713578331\n",
      "Epoch: 728 Loss: 106.59479756394272\n",
      "Epoch: 729 Loss: 106.59448906271632\n",
      "Epoch: 730 Loss: 106.59418162675908\n",
      "Epoch: 731 Loss: 106.59387525076005\n",
      "Epoch: 732 Loss: 106.59356992944203\n",
      "Epoch: 733 Loss: 106.59326565756152\n",
      "Epoch: 734 Loss: 106.59296242990823\n",
      "Epoch: 735 Loss: 106.59266024130494\n",
      "Epoch: 736 Loss: 106.59235908660723\n",
      "Epoch: 737 Loss: 106.59205896070321\n",
      "Epoch: 738 Loss: 106.59175985851329\n",
      "Epoch: 739 Loss: 106.59146177498985\n",
      "Epoch: 740 Loss: 106.59116470511714\n",
      "Epoch: 741 Loss: 106.59086864391095\n",
      "Epoch: 742 Loss: 106.59057358641834\n",
      "Epoch: 743 Loss: 106.59027952771747\n",
      "Epoch: 744 Loss: 106.58998646291731\n",
      "Epoch: 745 Loss: 106.58969438715741\n",
      "Epoch: 746 Loss: 106.58940329560775\n",
      "Epoch: 747 Loss: 106.58911318346838\n",
      "Epoch: 748 Loss: 106.58882404596923\n",
      "Epoch: 749 Loss: 106.58853587836997\n",
      "Epoch: 750 Loss: 106.58824867595969\n",
      "Epoch: 751 Loss: 106.5879624340567\n",
      "Epoch: 752 Loss: 106.58767714800831\n",
      "Epoch: 753 Loss: 106.58739281319068\n",
      "Epoch: 754 Loss: 106.5871094250084\n",
      "Epoch: 755 Loss: 106.58682697889462\n",
      "Epoch: 756 Loss: 106.58654547031045\n",
      "Epoch: 757 Loss: 106.58626489474503\n",
      "Epoch: 758 Loss: 106.58598524771513\n",
      "Epoch: 759 Loss: 106.58570652476517\n",
      "Epoch: 760 Loss: 106.58542872146681\n",
      "Epoch: 761 Loss: 106.58515183341878\n",
      "Epoch: 762 Loss: 106.58487585624673\n",
      "Epoch: 763 Loss: 106.58460078560304\n",
      "Epoch: 764 Loss: 106.58432661716661\n",
      "Epoch: 765 Loss: 106.58405334664262\n",
      "Epoch: 766 Loss: 106.5837809697623\n",
      "Epoch: 767 Loss: 106.58350948228292\n",
      "Epoch: 768 Loss: 106.58323887998742\n",
      "Epoch: 769 Loss: 106.58296915868422\n",
      "Epoch: 770 Loss: 106.5827003142072\n",
      "Epoch: 771 Loss: 106.58243234241536\n",
      "Epoch: 772 Loss: 106.58216523919259\n",
      "Epoch: 773 Loss: 106.58189900044769\n",
      "Epoch: 774 Loss: 106.58163362211404\n",
      "Epoch: 775 Loss: 106.58136910014946\n",
      "Epoch: 776 Loss: 106.58110543053596\n",
      "Epoch: 777 Loss: 106.5808426092797\n",
      "Epoch: 778 Loss: 106.58058063241067\n",
      "Epoch: 779 Loss: 106.58031949598269\n",
      "Epoch: 780 Loss: 106.58005919607305\n",
      "Epoch: 781 Loss: 106.57979972878242\n",
      "Epoch: 782 Loss: 106.57954109023473\n",
      "Epoch: 783 Loss: 106.57928327657689\n",
      "Epoch: 784 Loss: 106.57902628397875\n",
      "Epoch: 785 Loss: 106.57877010863288\n",
      "Epoch: 786 Loss: 106.57851474675432\n",
      "Epoch: 787 Loss: 106.57826019458057\n",
      "Epoch: 788 Loss: 106.57800644837128\n",
      "Epoch: 789 Loss: 106.57775350440826\n",
      "Epoch: 790 Loss: 106.57750135899518\n",
      "Epoch: 791 Loss: 106.5772500084574\n",
      "Epoch: 792 Loss: 106.57699944914198\n",
      "Epoch: 793 Loss: 106.57674967741731\n",
      "Epoch: 794 Loss: 106.57650068967315\n",
      "Epoch: 795 Loss: 106.5762524823204\n",
      "Epoch: 796 Loss: 106.57600505179086\n",
      "Epoch: 797 Loss: 106.57575839453719\n",
      "Epoch: 798 Loss: 106.57551250703277\n",
      "Epoch: 799 Loss: 106.57526738577155\n",
      "Epoch: 800 Loss: 106.57502302726775\n",
      "Epoch: 801 Loss: 106.57477942805598\n",
      "Epoch: 802 Loss: 106.57453658469078\n",
      "Epoch: 803 Loss: 106.57429449374688\n",
      "Epoch: 804 Loss: 106.57405315181866\n",
      "Epoch: 805 Loss: 106.57381255552023\n",
      "Epoch: 806 Loss: 106.57357270148522\n",
      "Epoch: 807 Loss: 106.57333358636676\n",
      "Epoch: 808 Loss: 106.57309520683711\n",
      "Epoch: 809 Loss: 106.57285755958779\n",
      "Epoch: 810 Loss: 106.57262064132925\n",
      "Epoch: 811 Loss: 106.57238444879086\n",
      "Epoch: 812 Loss: 106.57214897872062\n",
      "Epoch: 813 Loss: 106.57191422788527\n",
      "Epoch: 814 Loss: 106.57168019306997\n",
      "Epoch: 815 Loss: 106.57144687107822\n",
      "Epoch: 816 Loss: 106.57121425873171\n",
      "Epoch: 817 Loss: 106.57098235287025\n",
      "Epoch: 818 Loss: 106.57075115035167\n",
      "Epoch: 819 Loss: 106.57052064805158\n",
      "Epoch: 820 Loss: 106.57029084286332\n",
      "Epoch: 821 Loss: 106.57006173169788\n",
      "Epoch: 822 Loss: 106.5698333114836\n",
      "Epoch: 823 Loss: 106.56960557916635\n",
      "Epoch: 824 Loss: 106.5693785317091\n",
      "Epoch: 825 Loss: 106.56915216609201\n",
      "Epoch: 826 Loss: 106.56892647931218\n",
      "Epoch: 827 Loss: 106.56870146838367\n",
      "Epoch: 828 Loss: 106.56847713033727\n",
      "Epoch: 829 Loss: 106.5682534622204\n",
      "Epoch: 830 Loss: 106.56803046109705\n",
      "Epoch: 831 Loss: 106.56780812404762\n",
      "Epoch: 832 Loss: 106.5675864481689\n",
      "Epoch: 833 Loss: 106.56736543057372\n",
      "Epoch: 834 Loss: 106.5671450683912\n",
      "Epoch: 835 Loss: 106.5669253587663\n",
      "Epoch: 836 Loss: 106.56670629885996\n",
      "Epoch: 837 Loss: 106.56648788584879\n",
      "Epoch: 838 Loss: 106.5662701169251\n",
      "Epoch: 839 Loss: 106.56605298929688\n",
      "Epoch: 840 Loss: 106.56583650018732\n",
      "Epoch: 841 Loss: 106.56562064683516\n",
      "Epoch: 842 Loss: 106.56540542649432\n",
      "Epoch: 843 Loss: 106.56519083643389\n",
      "Epoch: 844 Loss: 106.56497687393795\n",
      "Epoch: 845 Loss: 106.56476353630556\n",
      "Epoch: 846 Loss: 106.5645508208506\n",
      "Epoch: 847 Loss: 106.5643387249017\n",
      "Epoch: 848 Loss: 106.56412724580213\n",
      "Epoch: 849 Loss: 106.56391638090967\n",
      "Epoch: 850 Loss: 106.56370612759666\n",
      "Epoch: 851 Loss: 106.56349648324964\n",
      "Epoch: 852 Loss: 106.56328744526947\n",
      "Epoch: 853 Loss: 106.56307901107124\n",
      "Epoch: 854 Loss: 106.56287117808401\n",
      "Epoch: 855 Loss: 106.56266394375083\n",
      "Epoch: 856 Loss: 106.56245730552871\n",
      "Epoch: 857 Loss: 106.56225126088833\n",
      "Epoch: 858 Loss: 106.56204580731416\n",
      "Epoch: 859 Loss: 106.56184094230422\n",
      "Epoch: 860 Loss: 106.56163666337015\n",
      "Epoch: 861 Loss: 106.56143296803684\n",
      "Epoch: 862 Loss: 106.5612298538427\n",
      "Epoch: 863 Loss: 106.56102731833933\n",
      "Epoch: 864 Loss: 106.56082535909144\n",
      "Epoch: 865 Loss: 106.5606239736769\n",
      "Epoch: 866 Loss: 106.56042315968656\n",
      "Epoch: 867 Loss: 106.5602229147241\n",
      "Epoch: 868 Loss: 106.56002323640617\n",
      "Epoch: 869 Loss: 106.55982412236207\n",
      "Epoch: 870 Loss: 106.55962557023373\n",
      "Epoch: 871 Loss: 106.55942757767576\n",
      "Epoch: 872 Loss: 106.55923014235518\n",
      "Epoch: 873 Loss: 106.55903326195147\n",
      "Epoch: 874 Loss: 106.55883693415642\n",
      "Epoch: 875 Loss: 106.55864115667404\n",
      "Epoch: 876 Loss: 106.55844592722063\n",
      "Epoch: 877 Loss: 106.55825124352448\n",
      "Epoch: 878 Loss: 106.55805710332594\n",
      "Epoch: 879 Loss: 106.55786350437728\n",
      "Epoch: 880 Loss: 106.55767044444266\n",
      "Epoch: 881 Loss: 106.55747792129806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 882 Loss: 106.55728593273108\n",
      "Epoch: 883 Loss: 106.55709447654101\n",
      "Epoch: 884 Loss: 106.55690355053873\n",
      "Epoch: 885 Loss: 106.55671315254658\n",
      "Epoch: 886 Loss: 106.55652328039828\n",
      "Epoch: 887 Loss: 106.55633393193897\n",
      "Epoch: 888 Loss: 106.556145105025\n",
      "Epoch: 889 Loss: 106.555956797524\n",
      "Epoch: 890 Loss: 106.55576900731458\n",
      "Epoch: 891 Loss: 106.55558173228653\n",
      "Epoch: 892 Loss: 106.5553949703406\n",
      "Epoch: 893 Loss: 106.55520871938845\n",
      "Epoch: 894 Loss: 106.5550229773526\n",
      "Epoch: 895 Loss: 106.55483774216628\n",
      "Epoch: 896 Loss: 106.55465301177357\n",
      "Epoch: 897 Loss: 106.55446878412903\n",
      "Epoch: 898 Loss: 106.55428505719794\n",
      "Epoch: 899 Loss: 106.55410182895605\n",
      "Epoch: 900 Loss: 106.55391909738944\n",
      "Epoch: 901 Loss: 106.55373686049475\n",
      "Epoch: 902 Loss: 106.5535551162788\n",
      "Epoch: 903 Loss: 106.55337386275875\n",
      "Epoch: 904 Loss: 106.55319309796187\n",
      "Epoch: 905 Loss: 106.55301281992556\n",
      "Epoch: 906 Loss: 106.5528330266973\n",
      "Epoch: 907 Loss: 106.55265371633465\n",
      "Epoch: 908 Loss: 106.55247488690493\n",
      "Epoch: 909 Loss: 106.55229653648546\n",
      "Epoch: 910 Loss: 106.5521186631633\n",
      "Epoch: 911 Loss: 106.55194126503532\n",
      "Epoch: 912 Loss: 106.55176434020801\n",
      "Epoch: 913 Loss: 106.55158788679755\n",
      "Epoch: 914 Loss: 106.55141190292967\n",
      "Epoch: 915 Loss: 106.55123638673959\n",
      "Epoch: 916 Loss: 106.551061336372\n",
      "Epoch: 917 Loss: 106.55088674998095\n",
      "Epoch: 918 Loss: 106.5507126257299\n",
      "Epoch: 919 Loss: 106.5505389617915\n",
      "Epoch: 920 Loss: 106.55036575634769\n",
      "Epoch: 921 Loss: 106.5501930075895\n",
      "Epoch: 922 Loss: 106.55002071371716\n",
      "Epoch: 923 Loss: 106.54984887293989\n",
      "Epoch: 924 Loss: 106.5496774834759\n",
      "Epoch: 925 Loss: 106.54950654355234\n",
      "Epoch: 926 Loss: 106.54933605140533\n",
      "Epoch: 927 Loss: 106.54916600527973\n",
      "Epoch: 928 Loss: 106.54899640342921\n",
      "Epoch: 929 Loss: 106.54882724411613\n",
      "Epoch: 930 Loss: 106.54865852561161\n",
      "Epoch: 931 Loss: 106.54849024619531\n",
      "Epoch: 932 Loss: 106.54832240415547\n",
      "Epoch: 933 Loss: 106.5481549977889\n",
      "Epoch: 934 Loss: 106.54798802540077\n",
      "Epoch: 935 Loss: 106.54782148530481\n",
      "Epoch: 936 Loss: 106.5476553758229\n",
      "Epoch: 937 Loss: 106.54748969528549\n",
      "Epoch: 938 Loss: 106.5473244420311\n",
      "Epoch: 939 Loss: 106.54715961440648\n",
      "Epoch: 940 Loss: 106.54699521076668\n",
      "Epoch: 941 Loss: 106.54683122947469\n",
      "Epoch: 942 Loss: 106.54666766890168\n",
      "Epoch: 943 Loss: 106.54650452742678\n",
      "Epoch: 944 Loss: 106.54634180343707\n",
      "Epoch: 945 Loss: 106.54617949532766\n",
      "Epoch: 946 Loss: 106.54601760150145\n",
      "Epoch: 947 Loss: 106.5458561203691\n",
      "Epoch: 948 Loss: 106.54569505034918\n",
      "Epoch: 949 Loss: 106.54553438986795\n",
      "Epoch: 950 Loss: 106.54537413735932\n",
      "Epoch: 951 Loss: 106.54521429126487\n",
      "Epoch: 952 Loss: 106.54505485003376\n",
      "Epoch: 953 Loss: 106.54489581212276\n",
      "Epoch: 954 Loss: 106.54473717599605\n",
      "Epoch: 955 Loss: 106.54457894012535\n",
      "Epoch: 956 Loss: 106.54442110298974\n",
      "Epoch: 957 Loss: 106.54426366307577\n",
      "Epoch: 958 Loss: 106.5441066188772\n",
      "Epoch: 959 Loss: 106.54394996889513\n",
      "Epoch: 960 Loss: 106.54379371163793\n",
      "Epoch: 961 Loss: 106.54363784562113\n",
      "Epoch: 962 Loss: 106.54348236936748\n",
      "Epoch: 963 Loss: 106.54332728140676\n",
      "Epoch: 964 Loss: 106.54317258027586\n",
      "Epoch: 965 Loss: 106.54301826451876\n",
      "Epoch: 966 Loss: 106.54286433268639\n",
      "Epoch: 967 Loss: 106.5427107833366\n",
      "Epoch: 968 Loss: 106.54255761503424\n",
      "Epoch: 969 Loss: 106.54240482635086\n",
      "Epoch: 970 Loss: 106.5422524158651\n",
      "Epoch: 971 Loss: 106.54210038216215\n",
      "Epoch: 972 Loss: 106.54194872383408\n",
      "Epoch: 973 Loss: 106.54179743947967\n",
      "Epoch: 974 Loss: 106.5416465277043\n",
      "Epoch: 975 Loss: 106.5414959871201\n",
      "Epoch: 976 Loss: 106.54134581634565\n",
      "Epoch: 977 Loss: 106.54119601400622\n",
      "Epoch: 978 Loss: 106.54104657873356\n",
      "Epoch: 979 Loss: 106.54089750916586\n",
      "Epoch: 980 Loss: 106.54074880394782\n",
      "Epoch: 981 Loss: 106.5406004617305\n",
      "Epoch: 982 Loss: 106.5404524811714\n",
      "Epoch: 983 Loss: 106.54030486093423\n",
      "Epoch: 984 Loss: 106.54015759968915\n",
      "Epoch: 985 Loss: 106.54001069611245\n",
      "Epoch: 986 Loss: 106.53986414888674\n",
      "Epoch: 987 Loss: 106.53971795670081\n",
      "Epoch: 988 Loss: 106.53957211824954\n",
      "Epoch: 989 Loss: 106.53942663223401\n",
      "Epoch: 990 Loss: 106.53928149736133\n",
      "Epoch: 991 Loss: 106.53913671234469\n",
      "Epoch: 992 Loss: 106.53899227590331\n",
      "Epoch: 993 Loss: 106.53884818676235\n",
      "Epoch: 994 Loss: 106.53870444365292\n",
      "Epoch: 995 Loss: 106.53856104531212\n",
      "Epoch: 996 Loss: 106.5384179904829\n",
      "Epoch: 997 Loss: 106.53827527791394\n",
      "Epoch: 998 Loss: 106.53813290635988\n",
      "Epoch: 999 Loss: 106.53799087458114\n",
      "Epoch: 1000 Loss: 106.53784918134377\n",
      "Epoch: 1001 Loss: 106.53770782541967\n",
      "Epoch: 1002 Loss: 106.53756680558634\n",
      "Epoch: 1003 Loss: 106.53742612062693\n",
      "Epoch: 1004 Loss: 106.53728576933034\n",
      "Epoch: 1005 Loss: 106.53714575049082\n",
      "Epoch: 1006 Loss: 106.5370060629084\n",
      "Epoch: 1007 Loss: 106.53686670538859\n",
      "Epoch: 1008 Loss: 106.53672767674229\n",
      "Epoch: 1009 Loss: 106.53658897578595\n",
      "Epoch: 1010 Loss: 106.53645060134147\n",
      "Epoch: 1011 Loss: 106.53631255223611\n",
      "Epoch: 1012 Loss: 106.53617482730252\n",
      "Epoch: 1013 Loss: 106.53603742537871\n",
      "Epoch: 1014 Loss: 106.53590034530794\n",
      "Epoch: 1015 Loss: 106.53576358593884\n",
      "Epoch: 1016 Loss: 106.53562714612528\n",
      "Epoch: 1017 Loss: 106.5354910247263\n",
      "Epoch: 1018 Loss: 106.53535522060619\n",
      "Epoch: 1019 Loss: 106.53521973263442\n",
      "Epoch: 1020 Loss: 106.53508455968552\n",
      "Epoch: 1021 Loss: 106.53494970063923\n",
      "Epoch: 1022 Loss: 106.53481515438028\n",
      "Epoch: 1023 Loss: 106.53468091979855\n",
      "Epoch: 1024 Loss: 106.53454699578889\n",
      "Epoch: 1025 Loss: 106.53441338125116\n",
      "Epoch: 1026 Loss: 106.53428007509014\n",
      "Epoch: 1027 Loss: 106.53414707621573\n",
      "Epoch: 1028 Loss: 106.53401438354251\n",
      "Epoch: 1029 Loss: 106.5338819959901\n",
      "Epoch: 1030 Loss: 106.53374991248302\n",
      "Epoch: 1031 Loss: 106.53361813195048\n",
      "Epoch: 1032 Loss: 106.53348665332658\n",
      "Epoch: 1033 Loss: 106.53335547555024\n",
      "Epoch: 1034 Loss: 106.53322459756512\n",
      "Epoch: 1035 Loss: 106.53309401831956\n",
      "Epoch: 1036 Loss: 106.53296373676667\n",
      "Epoch: 1037 Loss: 106.53283375186419\n",
      "Epoch: 1038 Loss: 106.53270406257455\n",
      "Epoch: 1039 Loss: 106.53257466786482\n",
      "Epoch: 1040 Loss: 106.53244556670664\n",
      "Epoch: 1041 Loss: 106.53231675807625\n",
      "Epoch: 1042 Loss: 106.53218824095444\n",
      "Epoch: 1043 Loss: 106.53206001432656\n",
      "Epoch: 1044 Loss: 106.5319320771824\n",
      "Epoch: 1045 Loss: 106.53180442851634\n",
      "Epoch: 1046 Loss: 106.53167706732708\n",
      "Epoch: 1047 Loss: 106.53154999261785\n",
      "Epoch: 1048 Loss: 106.53142320339632\n",
      "Epoch: 1049 Loss: 106.53129669867441\n",
      "Epoch: 1050 Loss: 106.53117047746854\n",
      "Epoch: 1051 Loss: 106.53104453879939\n",
      "Epoch: 1052 Loss: 106.53091888169202\n",
      "Epoch: 1053 Loss: 106.5307935051757\n",
      "Epoch: 1054 Loss: 106.530668408284\n",
      "Epoch: 1055 Loss: 106.53054359005483\n",
      "Epoch: 1056 Loss: 106.5304190495302\n",
      "Epoch: 1057 Loss: 106.53029478575635\n",
      "Epoch: 1058 Loss: 106.53017079778378\n",
      "Epoch: 1059 Loss: 106.53004708466703\n",
      "Epoch: 1060 Loss: 106.52992364546485\n",
      "Epoch: 1061 Loss: 106.5298004792401\n",
      "Epoch: 1062 Loss: 106.52967758505969\n",
      "Epoch: 1063 Loss: 106.52955496199466\n",
      "Epoch: 1064 Loss: 106.52943260912001\n",
      "Epoch: 1065 Loss: 106.52931052551489\n",
      "Epoch: 1066 Loss: 106.52918871026236\n",
      "Epoch: 1067 Loss: 106.52906716244948\n",
      "Epoch: 1068 Loss: 106.52894588116732\n",
      "Epoch: 1069 Loss: 106.52882486551079\n",
      "Epoch: 1070 Loss: 106.52870411457884\n",
      "Epoch: 1071 Loss: 106.52858362747423\n",
      "Epoch: 1072 Loss: 106.52846340330373\n",
      "Epoch: 1073 Loss: 106.52834344117778\n",
      "Epoch: 1074 Loss: 106.52822374021079\n",
      "Epoch: 1075 Loss: 106.528104299521\n",
      "Epoch: 1076 Loss: 106.52798511823038\n",
      "Epoch: 1077 Loss: 106.52786619546467\n",
      "Epoch: 1078 Loss: 106.52774753035345\n",
      "Epoch: 1079 Loss: 106.52762912203002\n",
      "Epoch: 1080 Loss: 106.52751096963132\n",
      "Epoch: 1081 Loss: 106.52739307229814\n",
      "Epoch: 1082 Loss: 106.52727542917472\n",
      "Epoch: 1083 Loss: 106.52715803940929\n",
      "Epoch: 1084 Loss: 106.52704090215339\n",
      "Epoch: 1085 Loss: 106.52692401656238\n",
      "Epoch: 1086 Loss: 106.5268073817952\n",
      "Epoch: 1087 Loss: 106.52669099701436\n",
      "Epoch: 1088 Loss: 106.52657486138591\n",
      "Epoch: 1089 Loss: 106.52645897407949\n",
      "Epoch: 1090 Loss: 106.52634333426828\n",
      "Epoch: 1091 Loss: 106.52622794112892\n",
      "Epoch: 1092 Loss: 106.52611279384158\n",
      "Epoch: 1093 Loss: 106.52599789158992\n",
      "Epoch: 1094 Loss: 106.52588323356105\n",
      "Epoch: 1095 Loss: 106.52576881894551\n",
      "Epoch: 1096 Loss: 106.52565464693723\n",
      "Epoch: 1097 Loss: 106.52554071673369\n",
      "Epoch: 1098 Loss: 106.52542702753557\n",
      "Epoch: 1099 Loss: 106.525313578547\n",
      "Epoch: 1100 Loss: 106.52520036897555\n",
      "Epoch: 1101 Loss: 106.52508739803199\n",
      "Epoch: 1102 Loss: 106.52497466493045\n",
      "Epoch: 1103 Loss: 106.52486216888848\n",
      "Epoch: 1104 Loss: 106.52474990912671\n",
      "Epoch: 1105 Loss: 106.52463788486922\n",
      "Epoch: 1106 Loss: 106.52452609534322\n",
      "Epoch: 1107 Loss: 106.52441453977923\n",
      "Epoch: 1108 Loss: 106.52430321741097\n",
      "Epoch: 1109 Loss: 106.5241921274754\n",
      "Epoch: 1110 Loss: 106.52408126921254\n",
      "Epoch: 1111 Loss: 106.52397064186576\n",
      "Epoch: 1112 Loss: 106.52386024468143\n",
      "Epoch: 1113 Loss: 106.52375007690917\n",
      "Epoch: 1114 Loss: 106.5236401378016\n",
      "Epoch: 1115 Loss: 106.52353042661461\n",
      "Epoch: 1116 Loss: 106.52342094260706\n",
      "Epoch: 1117 Loss: 106.5233116850409\n",
      "Epoch: 1118 Loss: 106.52320265318119\n",
      "Epoch: 1119 Loss: 106.52309384629599\n",
      "Epoch: 1120 Loss: 106.52298526365641\n",
      "Epoch: 1121 Loss: 106.52287690453659\n",
      "Epoch: 1122 Loss: 106.52276876821358\n",
      "Epoch: 1123 Loss: 106.52266085396761\n",
      "Epoch: 1124 Loss: 106.5225531610816\n",
      "Epoch: 1125 Loss: 106.52244568884171\n",
      "Epoch: 1126 Loss: 106.52233843653683\n",
      "Epoch: 1127 Loss: 106.5222314034589\n",
      "Epoch: 1128 Loss: 106.5221245889027\n",
      "Epoch: 1129 Loss: 106.52201799216587\n",
      "Epoch: 1130 Loss: 106.52191161254909\n",
      "Epoch: 1131 Loss: 106.52180544935574\n",
      "Epoch: 1132 Loss: 106.52169950189216\n",
      "Epoch: 1133 Loss: 106.52159376946749\n",
      "Epoch: 1134 Loss: 106.52148825139363\n",
      "Epoch: 1135 Loss: 106.52138294698544\n",
      "Epoch: 1136 Loss: 106.52127785556044\n",
      "Epoch: 1137 Loss: 106.52117297643895\n",
      "Epoch: 1138 Loss: 106.52106830894415\n",
      "Epoch: 1139 Loss: 106.52096385240189\n",
      "Epoch: 1140 Loss: 106.52085960614082\n",
      "Epoch: 1141 Loss: 106.52075556949224\n",
      "Epoch: 1142 Loss: 106.52065174179022\n",
      "Epoch: 1143 Loss: 106.5205481223716\n",
      "Epoch: 1144 Loss: 106.52044471057565\n",
      "Epoch: 1145 Loss: 106.52034150574467\n",
      "Epoch: 1146 Loss: 106.52023850722337\n",
      "Epoch: 1147 Loss: 106.5201357143592\n",
      "Epoch: 1148 Loss: 106.52003312650223\n",
      "Epoch: 1149 Loss: 106.51993074300515\n",
      "Epoch: 1150 Loss: 106.51982856322323\n",
      "Epoch: 1151 Loss: 106.5197265865144\n",
      "Epoch: 1152 Loss: 106.51962481223913\n",
      "Epoch: 1153 Loss: 106.51952323976052\n",
      "Epoch: 1154 Loss: 106.51942186844413\n",
      "Epoch: 1155 Loss: 106.51932069765813\n",
      "Epoch: 1156 Loss: 106.51921972677322\n",
      "Epoch: 1157 Loss: 106.51911895516258\n",
      "Epoch: 1158 Loss: 106.51901838220202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1159 Loss: 106.5189180072697\n",
      "Epoch: 1160 Loss: 106.5188178297463\n",
      "Epoch: 1161 Loss: 106.51871784901508\n",
      "Epoch: 1162 Loss: 106.5186180644616\n",
      "Epoch: 1163 Loss: 106.51851847547404\n",
      "Epoch: 1164 Loss: 106.51841908144284\n",
      "Epoch: 1165 Loss: 106.51831988176102\n",
      "Epoch: 1166 Loss: 106.51822087582391\n",
      "Epoch: 1167 Loss: 106.51812206302927\n",
      "Epoch: 1168 Loss: 106.51802344277729\n",
      "Epoch: 1169 Loss: 106.5179250144705\n",
      "Epoch: 1170 Loss: 106.51782677751382\n",
      "Epoch: 1171 Loss: 106.51772873131448\n",
      "Epoch: 1172 Loss: 106.51763087528212\n",
      "Epoch: 1173 Loss: 106.51753320882867\n",
      "Epoch: 1174 Loss: 106.51743573136835\n",
      "Epoch: 1175 Loss: 106.51733844231782\n",
      "Epoch: 1176 Loss: 106.51724134109588\n",
      "Epoch: 1177 Loss: 106.51714442712377\n",
      "Epoch: 1178 Loss: 106.51704769982486\n",
      "Epoch: 1179 Loss: 106.5169511586249\n",
      "Epoch: 1180 Loss: 106.51685480295184\n",
      "Epoch: 1181 Loss: 106.51675863223589\n",
      "Epoch: 1182 Loss: 106.51666264590953\n",
      "Epoch: 1183 Loss: 106.51656684340743\n",
      "Epoch: 1184 Loss: 106.51647122416644\n",
      "Epoch: 1185 Loss: 106.51637578762566\n",
      "Epoch: 1186 Loss: 106.51628053322639\n",
      "Epoch: 1187 Loss: 106.51618546041209\n",
      "Epoch: 1188 Loss: 106.51609056862839\n",
      "Epoch: 1189 Loss: 106.51599585732308\n",
      "Epoch: 1190 Loss: 106.51590132594613\n",
      "Epoch: 1191 Loss: 106.51580697394962\n",
      "Epoch: 1192 Loss: 106.51571280078777\n",
      "Epoch: 1193 Loss: 106.51561880591697\n",
      "Epoch: 1194 Loss: 106.51552498879562\n",
      "Epoch: 1195 Loss: 106.51543134888429\n",
      "Epoch: 1196 Loss: 106.51533788564565\n",
      "Epoch: 1197 Loss: 106.51524459854441\n",
      "Epoch: 1198 Loss: 106.51515148704739\n",
      "Epoch: 1199 Loss: 106.51505855062342\n",
      "Epoch: 1200 Loss: 106.51496578874344\n",
      "Epoch: 1201 Loss: 106.51487320088044\n",
      "Epoch: 1202 Loss: 106.51478078650936\n",
      "Epoch: 1203 Loss: 106.51468854510725\n",
      "Epoch: 1204 Loss: 106.51459647615312\n",
      "Epoch: 1205 Loss: 106.51450457912803\n",
      "Epoch: 1206 Loss: 106.51441285351497\n",
      "Epoch: 1207 Loss: 106.51432129879902\n",
      "Epoch: 1208 Loss: 106.5142299144671\n",
      "Epoch: 1209 Loss: 106.51413870000825\n",
      "Epoch: 1210 Loss: 106.5140476549133\n",
      "Epoch: 1211 Loss: 106.51395677867518\n",
      "Epoch: 1212 Loss: 106.51386607078871\n",
      "Epoch: 1213 Loss: 106.51377553075059\n",
      "Epoch: 1214 Loss: 106.51368515805952\n",
      "Epoch: 1215 Loss: 106.51359495221604\n",
      "Epoch: 1216 Loss: 106.51350491272265\n",
      "Epoch: 1217 Loss: 106.51341503908375\n",
      "Epoch: 1218 Loss: 106.51332533080556\n",
      "Epoch: 1219 Loss: 106.51323578739625\n",
      "Epoch: 1220 Loss: 106.51314640836583\n",
      "Epoch: 1221 Loss: 106.51305719322616\n",
      "Epoch: 1222 Loss: 106.51296814149097\n",
      "Epoch: 1223 Loss: 106.51287925267584\n",
      "Epoch: 1224 Loss: 106.51279052629819\n",
      "Epoch: 1225 Loss: 106.51270196187723\n",
      "Epoch: 1226 Loss: 106.51261355893399\n",
      "Epoch: 1227 Loss: 106.51252531699137\n",
      "Epoch: 1228 Loss: 106.51243723557403\n",
      "Epoch: 1229 Loss: 106.51234931420842\n",
      "Epoch: 1230 Loss: 106.51226155242273\n",
      "Epoch: 1231 Loss: 106.51217394974707\n",
      "Epoch: 1232 Loss: 106.51208650571316\n",
      "Epoch: 1233 Loss: 106.51199921985459\n",
      "Epoch: 1234 Loss: 106.51191209170662\n",
      "Epoch: 1235 Loss: 106.51182512080634\n",
      "Epoch: 1236 Loss: 106.51173830669252\n",
      "Epoch: 1237 Loss: 106.51165164890568\n",
      "Epoch: 1238 Loss: 106.51156514698806\n",
      "Epoch: 1239 Loss: 106.51147880048359\n",
      "Epoch: 1240 Loss: 106.51139260893791\n",
      "Epoch: 1241 Loss: 106.51130657189844\n",
      "Epoch: 1242 Loss: 106.51122068891416\n",
      "Epoch: 1243 Loss: 106.5111349595358\n",
      "Epoch: 1244 Loss: 106.5110493833158\n",
      "Epoch: 1245 Loss: 106.51096395980822\n",
      "Epoch: 1246 Loss: 106.51087868856875\n",
      "Epoch: 1247 Loss: 106.51079356915479\n",
      "Epoch: 1248 Loss: 106.51070860112539\n",
      "Epoch: 1249 Loss: 106.51062378404119\n",
      "Epoch: 1250 Loss: 106.5105391174645\n",
      "Epoch: 1251 Loss: 106.51045460095922\n",
      "Epoch: 1252 Loss: 106.51037023409089\n",
      "Epoch: 1253 Loss: 106.51028601642668\n",
      "Epoch: 1254 Loss: 106.51020194753525\n",
      "Epoch: 1255 Loss: 106.51011802698699\n",
      "Epoch: 1256 Loss: 106.51003425435387\n",
      "Epoch: 1257 Loss: 106.50995062920931\n",
      "Epoch: 1258 Loss: 106.5098671511284\n",
      "Epoch: 1259 Loss: 106.50978381968784\n",
      "Epoch: 1260 Loss: 106.50970063446574\n",
      "Epoch: 1261 Loss: 106.50961759504193\n",
      "Epoch: 1262 Loss: 106.50953470099768\n",
      "Epoch: 1263 Loss: 106.5094519519158\n",
      "Epoch: 1264 Loss: 106.50936934738067\n",
      "Epoch: 1265 Loss: 106.50928688697816\n",
      "Epoch: 1266 Loss: 106.50920457029572\n",
      "Epoch: 1267 Loss: 106.50912239692224\n",
      "Epoch: 1268 Loss: 106.50904036644813\n",
      "Epoch: 1269 Loss: 106.50895847846532\n",
      "Epoch: 1270 Loss: 106.50887673256719\n",
      "Epoch: 1271 Loss: 106.50879512834868\n",
      "Epoch: 1272 Loss: 106.50871366540615\n",
      "Epoch: 1273 Loss: 106.50863234333741\n",
      "Epoch: 1274 Loss: 106.50855116174182\n",
      "Epoch: 1275 Loss: 106.50847012022004\n",
      "Epoch: 1276 Loss: 106.5083892183744\n",
      "Epoch: 1277 Loss: 106.50830845580849\n",
      "Epoch: 1278 Loss: 106.50822783212742\n",
      "Epoch: 1279 Loss: 106.50814734693772\n",
      "Epoch: 1280 Loss: 106.50806699984739\n",
      "Epoch: 1281 Loss: 106.50798679046574\n",
      "Epoch: 1282 Loss: 106.50790671840358\n",
      "Epoch: 1283 Loss: 106.50782678327315\n",
      "Epoch: 1284 Loss: 106.50774698468798\n",
      "Epoch: 1285 Loss: 106.50766732226315\n",
      "Epoch: 1286 Loss: 106.50758779561494\n",
      "Epoch: 1287 Loss: 106.50750840436119\n",
      "Epoch: 1288 Loss: 106.50742914812105\n",
      "Epoch: 1289 Loss: 106.50735002651497\n",
      "Epoch: 1290 Loss: 106.50727103916488\n",
      "Epoch: 1291 Loss: 106.50719218569398\n",
      "Epoch: 1292 Loss: 106.50711346572693\n",
      "Epoch: 1293 Loss: 106.50703487888958\n",
      "Epoch: 1294 Loss: 106.50695642480927\n",
      "Epoch: 1295 Loss: 106.50687810311457\n",
      "Epoch: 1296 Loss: 106.50679991343549\n",
      "Epoch: 1297 Loss: 106.50672185540321\n",
      "Epoch: 1298 Loss: 106.5066439286504\n",
      "Epoch: 1299 Loss: 106.50656613281093\n",
      "Epoch: 1300 Loss: 106.50648846751994\n",
      "Epoch: 1301 Loss: 106.50641093241401\n",
      "Epoch: 1302 Loss: 106.50633352713092\n",
      "Epoch: 1303 Loss: 106.50625625130981\n",
      "Epoch: 1304 Loss: 106.50617910459097\n",
      "Epoch: 1305 Loss: 106.50610208661611\n",
      "Epoch: 1306 Loss: 106.50602519702814\n",
      "Epoch: 1307 Loss: 106.50594843547124\n",
      "Epoch: 1308 Loss: 106.5058718015909\n",
      "Epoch: 1309 Loss: 106.50579529503383\n",
      "Epoch: 1310 Loss: 106.505718915448\n",
      "Epoch: 1311 Loss: 106.50564266248259\n",
      "Epoch: 1312 Loss: 106.50556653578809\n",
      "Epoch: 1313 Loss: 106.50549053501615\n",
      "Epoch: 1314 Loss: 106.50541465981973\n",
      "Epoch: 1315 Loss: 106.50533890985295\n",
      "Epoch: 1316 Loss: 106.50526328477117\n",
      "Epoch: 1317 Loss: 106.505187784231\n",
      "Epoch: 1318 Loss: 106.50511240789018\n",
      "Epoch: 1319 Loss: 106.50503715540772\n",
      "Epoch: 1320 Loss: 106.50496202644383\n",
      "Epoch: 1321 Loss: 106.50488702065988\n",
      "Epoch: 1322 Loss: 106.50481213771842\n",
      "Epoch: 1323 Loss: 106.50473737728326\n",
      "Epoch: 1324 Loss: 106.50466273901927\n",
      "Epoch: 1325 Loss: 106.50458822259262\n",
      "Epoch: 1326 Loss: 106.50451382767054\n",
      "Epoch: 1327 Loss: 106.5044395539215\n",
      "Epoch: 1328 Loss: 106.50436540101506\n",
      "Epoch: 1329 Loss: 106.50429136862205\n",
      "Epoch: 1330 Loss: 106.50421745641431\n",
      "Epoch: 1331 Loss: 106.50414366406491\n",
      "Epoch: 1332 Loss: 106.50406999124803\n",
      "Epoch: 1333 Loss: 106.50399643763895\n",
      "Epoch: 1334 Loss: 106.5039230029142\n",
      "Epoch: 1335 Loss: 106.50384968675131\n",
      "Epoch: 1336 Loss: 106.50377648882899\n",
      "Epoch: 1337 Loss: 106.50370340882704\n",
      "Epoch: 1338 Loss: 106.50363044642637\n",
      "Epoch: 1339 Loss: 106.50355760130904\n",
      "Epoch: 1340 Loss: 106.50348487315817\n",
      "Epoch: 1341 Loss: 106.50341226165799\n",
      "Epoch: 1342 Loss: 106.50333976649377\n",
      "Epoch: 1343 Loss: 106.50326738735198\n",
      "Epoch: 1344 Loss: 106.50319512392008\n",
      "Epoch: 1345 Loss: 106.50312297588665\n",
      "Epoch: 1346 Loss: 106.50305094294133\n",
      "Epoch: 1347 Loss: 106.50297902477485\n",
      "Epoch: 1348 Loss: 106.50290722107891\n",
      "Epoch: 1349 Loss: 106.50283553154642\n",
      "Epoch: 1350 Loss: 106.50276395587127\n",
      "Epoch: 1351 Loss: 106.50269249374841\n",
      "Epoch: 1352 Loss: 106.50262114487379\n",
      "Epoch: 1353 Loss: 106.50254990894449\n",
      "Epoch: 1354 Loss: 106.50247878565857\n",
      "Epoch: 1355 Loss: 106.50240777471514\n",
      "Epoch: 1356 Loss: 106.50233687581431\n",
      "Epoch: 1357 Loss: 106.5022660886573\n",
      "Epoch: 1358 Loss: 106.50219541294628\n",
      "Epoch: 1359 Loss: 106.50212484838444\n",
      "Epoch: 1360 Loss: 106.50205439467601\n",
      "Epoch: 1361 Loss: 106.50198405152621\n",
      "Epoch: 1362 Loss: 106.50191381864128\n",
      "Epoch: 1363 Loss: 106.50184369572844\n",
      "Epoch: 1364 Loss: 106.50177368249595\n",
      "Epoch: 1365 Loss: 106.501703778653\n",
      "Epoch: 1366 Loss: 106.50163398390984\n",
      "Epoch: 1367 Loss: 106.5015642979776\n",
      "Epoch: 1368 Loss: 106.5014947205685\n",
      "Epoch: 1369 Loss: 106.50142525139569\n",
      "Epoch: 1370 Loss: 106.5013558901733\n",
      "Epoch: 1371 Loss: 106.50128663661641\n",
      "Epoch: 1372 Loss: 106.50121749044105\n",
      "Epoch: 1373 Loss: 106.5011484513643\n",
      "Epoch: 1374 Loss: 106.50107951910408\n",
      "Epoch: 1375 Loss: 106.50101069337931\n",
      "Epoch: 1376 Loss: 106.50094197390989\n",
      "Epoch: 1377 Loss: 106.50087336041666\n",
      "Epoch: 1378 Loss: 106.50080485262136\n",
      "Epoch: 1379 Loss: 106.50073645024665\n",
      "Epoch: 1380 Loss: 106.5006681530162\n",
      "Epoch: 1381 Loss: 106.50059996065457\n",
      "Epoch: 1382 Loss: 106.50053187288721\n",
      "Epoch: 1383 Loss: 106.5004638894406\n",
      "Epoch: 1384 Loss: 106.50039601004198\n",
      "Epoch: 1385 Loss: 106.50032823441963\n",
      "Epoch: 1386 Loss: 106.50026056230267\n",
      "Epoch: 1387 Loss: 106.50019299342122\n",
      "Epoch: 1388 Loss: 106.50012552750619\n",
      "Epoch: 1389 Loss: 106.50005816428944\n",
      "Epoch: 1390 Loss: 106.49999090350376\n",
      "Epoch: 1391 Loss: 106.49992374488275\n",
      "Epoch: 1392 Loss: 106.49985668816099\n",
      "Epoch: 1393 Loss: 106.49978973307388\n",
      "Epoch: 1394 Loss: 106.4997228793577\n",
      "Epoch: 1395 Loss: 106.49965612674968\n",
      "Epoch: 1396 Loss: 106.49958947498784\n",
      "Epoch: 1397 Loss: 106.49952292381113\n",
      "Epoch: 1398 Loss: 106.49945647295938\n",
      "Epoch: 1399 Loss: 106.49939012217311\n",
      "Epoch: 1400 Loss: 106.49932387119401\n",
      "Epoch: 1401 Loss: 106.49925771976432\n",
      "Epoch: 1402 Loss: 106.49919166762736\n",
      "Epoch: 1403 Loss: 106.49912571452718\n",
      "Epoch: 1404 Loss: 106.49905986020867\n",
      "Epoch: 1405 Loss: 106.49899410441765\n",
      "Epoch: 1406 Loss: 106.49892844690072\n",
      "Epoch: 1407 Loss: 106.4988628874053\n",
      "Epoch: 1408 Loss: 106.49879742567971\n",
      "Epoch: 1409 Loss: 106.49873206147304\n",
      "Epoch: 1410 Loss: 106.49866679453518\n",
      "Epoch: 1411 Loss: 106.49860162461695\n",
      "Epoch: 1412 Loss: 106.49853655146991\n",
      "Epoch: 1413 Loss: 106.49847157484646\n",
      "Epoch: 1414 Loss: 106.49840669449983\n",
      "Epoch: 1415 Loss: 106.498341910184\n",
      "Epoch: 1416 Loss: 106.49827722165381\n",
      "Epoch: 1417 Loss: 106.4982126286649\n",
      "Epoch: 1418 Loss: 106.49814813097369\n",
      "Epoch: 1419 Loss: 106.49808372833745\n",
      "Epoch: 1420 Loss: 106.49801942051413\n",
      "Epoch: 1421 Loss: 106.49795520726258\n",
      "Epoch: 1422 Loss: 106.49789108834246\n",
      "Epoch: 1423 Loss: 106.49782706351408\n",
      "Epoch: 1424 Loss: 106.49776313253864\n",
      "Epoch: 1425 Loss: 106.49769929517808\n",
      "Epoch: 1426 Loss: 106.49763555119513\n",
      "Epoch: 1427 Loss: 106.49757190035328\n",
      "Epoch: 1428 Loss: 106.49750834241682\n",
      "Epoch: 1429 Loss: 106.49744487715073\n",
      "Epoch: 1430 Loss: 106.49738150432091\n",
      "Epoch: 1431 Loss: 106.4973182236938\n",
      "Epoch: 1432 Loss: 106.49725503503679\n",
      "Epoch: 1433 Loss: 106.49719193811788\n",
      "Epoch: 1434 Loss: 106.49712893270598\n",
      "Epoch: 1435 Loss: 106.4970660185706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1436 Loss: 106.4970031954821\n",
      "Epoch: 1437 Loss: 106.49694046321149\n",
      "Epoch: 1438 Loss: 106.49687782153059\n",
      "Epoch: 1439 Loss: 106.4968152702119\n",
      "Epoch: 1440 Loss: 106.49675280902873\n",
      "Epoch: 1441 Loss: 106.49669043775512\n",
      "Epoch: 1442 Loss: 106.49662815616571\n",
      "Epoch: 1443 Loss: 106.49656596403601\n",
      "Epoch: 1444 Loss: 106.49650386114219\n",
      "Epoch: 1445 Loss: 106.49644184726114\n",
      "Epoch: 1446 Loss: 106.49637992217046\n",
      "Epoch: 1447 Loss: 106.4963180856485\n",
      "Epoch: 1448 Loss: 106.4962563374743\n",
      "Epoch: 1449 Loss: 106.49619467742758\n",
      "Epoch: 1450 Loss: 106.49613310528888\n",
      "Epoch: 1451 Loss: 106.49607162083923\n",
      "Epoch: 1452 Loss: 106.49601022386061\n",
      "Epoch: 1453 Loss: 106.4959489141355\n",
      "Epoch: 1454 Loss: 106.49588769144717\n",
      "Epoch: 1455 Loss: 106.49582655557957\n",
      "Epoch: 1456 Loss: 106.49576550631735\n",
      "Epoch: 1457 Loss: 106.49570454344578\n",
      "Epoch: 1458 Loss: 106.49564366675095\n",
      "Epoch: 1459 Loss: 106.49558287601947\n",
      "Epoch: 1460 Loss: 106.49552217103874\n",
      "Epoch: 1461 Loss: 106.49546155159682\n",
      "Epoch: 1462 Loss: 106.49540101748241\n",
      "Epoch: 1463 Loss: 106.49534056848485\n",
      "Epoch: 1464 Loss: 106.4952802043943\n",
      "Epoch: 1465 Loss: 106.49521992500138\n",
      "Epoch: 1466 Loss: 106.49515973009758\n",
      "Epoch: 1467 Loss: 106.49509961947487\n",
      "Epoch: 1468 Loss: 106.49503959292599\n",
      "Epoch: 1469 Loss: 106.49497965024428\n",
      "Epoch: 1470 Loss: 106.49491979122381\n",
      "Epoch: 1471 Loss: 106.4948600156592\n",
      "Epoch: 1472 Loss: 106.49480032334577\n",
      "Epoch: 1473 Loss: 106.4947407140795\n",
      "Epoch: 1474 Loss: 106.49468118765698\n",
      "Epoch: 1475 Loss: 106.49462174387548\n",
      "Epoch: 1476 Loss: 106.49456238253289\n",
      "Epoch: 1477 Loss: 106.49450310342773\n",
      "Epoch: 1478 Loss: 106.49444390635911\n",
      "Epoch: 1479 Loss: 106.49438479112685\n",
      "Epoch: 1480 Loss: 106.4943257575314\n",
      "Epoch: 1481 Loss: 106.49426680537377\n",
      "Epoch: 1482 Loss: 106.49420793445564\n",
      "Epoch: 1483 Loss: 106.49414914457927\n",
      "Epoch: 1484 Loss: 106.49409043554762\n",
      "Epoch: 1485 Loss: 106.49403180716418\n",
      "Epoch: 1486 Loss: 106.4939732592331\n",
      "Epoch: 1487 Loss: 106.4939147915591\n",
      "Epoch: 1488 Loss: 106.49385640394765\n",
      "Epoch: 1489 Loss: 106.49379809620464\n",
      "Epoch: 1490 Loss: 106.49373986813663\n",
      "Epoch: 1491 Loss: 106.49368171955085\n",
      "Epoch: 1492 Loss: 106.49362365025507\n",
      "Epoch: 1493 Loss: 106.49356566005768\n",
      "Epoch: 1494 Loss: 106.49350774876764\n",
      "Epoch: 1495 Loss: 106.49344991619454\n",
      "Epoch: 1496 Loss: 106.49339216214854\n",
      "Epoch: 1497 Loss: 106.49333448644038\n",
      "Epoch: 1498 Loss: 106.49327688888141\n",
      "Epoch: 1499 Loss: 106.49321936928354\n",
      "Epoch: 1500 Loss: 106.49316192745934\n",
      "Epoch: 1501 Loss: 106.49310456322183\n",
      "Epoch: 1502 Loss: 106.49304727638474\n",
      "Epoch: 1503 Loss: 106.49299006676227\n",
      "Epoch: 1504 Loss: 106.49293293416927\n",
      "Epoch: 1505 Loss: 106.49287587842112\n",
      "Epoch: 1506 Loss: 106.4928188993338\n",
      "Epoch: 1507 Loss: 106.4927619967238\n",
      "Epoch: 1508 Loss: 106.49270517040833\n",
      "Epoch: 1509 Loss: 106.4926484202049\n",
      "Epoch: 1510 Loss: 106.49259174593183\n",
      "Epoch: 1511 Loss: 106.49253514740788\n",
      "Epoch: 1512 Loss: 106.49247862445239\n",
      "Epoch: 1513 Loss: 106.49242217688528\n",
      "Epoch: 1514 Loss: 106.49236580452695\n",
      "Epoch: 1515 Loss: 106.49230950719846\n",
      "Epoch: 1516 Loss: 106.4922532847213\n",
      "Epoch: 1517 Loss: 106.49219713691762\n",
      "Epoch: 1518 Loss: 106.49214106361009\n",
      "Epoch: 1519 Loss: 106.4920850646218\n",
      "Epoch: 1520 Loss: 106.49202913977653\n",
      "Epoch: 1521 Loss: 106.4919732888986\n",
      "Epoch: 1522 Loss: 106.49191751181273\n",
      "Epoch: 1523 Loss: 106.49186180834431\n",
      "Epoch: 1524 Loss: 106.49180617831917\n",
      "Epoch: 1525 Loss: 106.49175062156378\n",
      "Epoch: 1526 Loss: 106.49169513790501\n",
      "Epoch: 1527 Loss: 106.49163972717037\n",
      "Epoch: 1528 Loss: 106.4915843891878\n",
      "Epoch: 1529 Loss: 106.49152912378588\n",
      "Epoch: 1530 Loss: 106.49147393079357\n",
      "Epoch: 1531 Loss: 106.49141881004044\n",
      "Epoch: 1532 Loss: 106.4913637613566\n",
      "Epoch: 1533 Loss: 106.4913087845726\n",
      "Epoch: 1534 Loss: 106.49125387951955\n",
      "Epoch: 1535 Loss: 106.49119904602905\n",
      "Epoch: 1536 Loss: 106.49114428393321\n",
      "Epoch: 1537 Loss: 106.49108959306469\n",
      "Epoch: 1538 Loss: 106.49103497325665\n",
      "Epoch: 1539 Loss: 106.49098042434267\n",
      "Epoch: 1540 Loss: 106.49092594615693\n",
      "Epoch: 1541 Loss: 106.49087153853407\n",
      "Epoch: 1542 Loss: 106.49081720130924\n",
      "Epoch: 1543 Loss: 106.49076293431807\n",
      "Epoch: 1544 Loss: 106.49070873739669\n",
      "Epoch: 1545 Loss: 106.49065461038174\n",
      "Epoch: 1546 Loss: 106.49060055311038\n",
      "Epoch: 1547 Loss: 106.49054656542016\n",
      "Epoch: 1548 Loss: 106.4904926471492\n",
      "Epoch: 1549 Loss: 106.49043879813613\n",
      "Epoch: 1550 Loss: 106.49038501821997\n",
      "Epoch: 1551 Loss: 106.4903313072403\n",
      "Epoch: 1552 Loss: 106.49027766503717\n",
      "Epoch: 1553 Loss: 106.49022409145103\n",
      "Epoch: 1554 Loss: 106.49017058632298\n",
      "Epoch: 1555 Loss: 106.49011714949438\n",
      "Epoch: 1556 Loss: 106.49006378080728\n",
      "Epoch: 1557 Loss: 106.490010480104\n",
      "Epoch: 1558 Loss: 106.4899572472275\n",
      "Epoch: 1559 Loss: 106.4899040820211\n",
      "Epoch: 1560 Loss: 106.48985098432863\n",
      "Epoch: 1561 Loss: 106.48979795399441\n",
      "Epoch: 1562 Loss: 106.48974499086314\n",
      "Epoch: 1563 Loss: 106.48969209478007\n",
      "Epoch: 1564 Loss: 106.4896392655909\n",
      "Epoch: 1565 Loss: 106.48958650314172\n",
      "Epoch: 1566 Loss: 106.48953380727916\n",
      "Epoch: 1567 Loss: 106.48948117785024\n",
      "Epoch: 1568 Loss: 106.4894286147025\n",
      "Epoch: 1569 Loss: 106.48937611768389\n",
      "Epoch: 1570 Loss: 106.4893236866428\n",
      "Epoch: 1571 Loss: 106.48927132142809\n",
      "Epoch: 1572 Loss: 106.4892190218891\n",
      "Epoch: 1573 Loss: 106.48916678787555\n",
      "Epoch: 1574 Loss: 106.48911461923758\n",
      "Epoch: 1575 Loss: 106.48906251582595\n",
      "Epoch: 1576 Loss: 106.48901047749165\n",
      "Epoch: 1577 Loss: 106.48895850408626\n",
      "Epoch: 1578 Loss: 106.48890659546169\n",
      "Epoch: 1579 Loss: 106.48885475147034\n",
      "Epoch: 1580 Loss: 106.48880297196509\n",
      "Epoch: 1581 Loss: 106.48875125679913\n",
      "Epoch: 1582 Loss: 106.48869960582624\n",
      "Epoch: 1583 Loss: 106.48864801890045\n",
      "Epoch: 1584 Loss: 106.4885964958764\n",
      "Epoch: 1585 Loss: 106.48854503660901\n",
      "Epoch: 1586 Loss: 106.48849364095375\n",
      "Epoch: 1587 Loss: 106.48844230876644\n",
      "Epoch: 1588 Loss: 106.48839103990325\n",
      "Epoch: 1589 Loss: 106.48833983422097\n",
      "Epoch: 1590 Loss: 106.48828869157666\n",
      "Epoch: 1591 Loss: 106.48823761182778\n",
      "Epoch: 1592 Loss: 106.48818659483231\n",
      "Epoch: 1593 Loss: 106.48813564044866\n",
      "Epoch: 1594 Loss: 106.48808474853547\n",
      "Epoch: 1595 Loss: 106.488033918952\n",
      "Epoch: 1596 Loss: 106.4879831515578\n",
      "Epoch: 1597 Loss: 106.48793244621288\n",
      "Epoch: 1598 Loss: 106.48788180277766\n",
      "Epoch: 1599 Loss: 106.4878312211129\n",
      "Epoch: 1600 Loss: 106.48778070107986\n",
      "Epoch: 1601 Loss: 106.48773024254011\n",
      "Epoch: 1602 Loss: 106.48767984535574\n",
      "Epoch: 1603 Loss: 106.48762950938915\n",
      "Epoch: 1604 Loss: 106.48757923450313\n",
      "Epoch: 1605 Loss: 106.4875290205609\n",
      "Epoch: 1606 Loss: 106.48747886742615\n",
      "Epoch: 1607 Loss: 106.48742877496281\n",
      "Epoch: 1608 Loss: 106.48737874303532\n",
      "Epoch: 1609 Loss: 106.48732877150849\n",
      "Epoch: 1610 Loss: 106.48727886024747\n",
      "Epoch: 1611 Loss: 106.48722900911794\n",
      "Epoch: 1612 Loss: 106.48717921798576\n",
      "Epoch: 1613 Loss: 106.4871294867173\n",
      "Epoch: 1614 Loss: 106.48707981517937\n",
      "Epoch: 1615 Loss: 106.48703020323904\n",
      "Epoch: 1616 Loss: 106.48698065076385\n",
      "Epoch: 1617 Loss: 106.48693115762168\n",
      "Epoch: 1618 Loss: 106.48688172368081\n",
      "Epoch: 1619 Loss: 106.48683234880988\n",
      "Epoch: 1620 Loss: 106.48678303287792\n",
      "Epoch: 1621 Loss: 106.48673377575436\n",
      "Epoch: 1622 Loss: 106.48668457730894\n",
      "Epoch: 1623 Loss: 106.48663543741188\n",
      "Epoch: 1624 Loss: 106.48658635593364\n",
      "Epoch: 1625 Loss: 106.48653733274516\n",
      "Epoch: 1626 Loss: 106.4864883677177\n",
      "Epoch: 1627 Loss: 106.48643946072289\n",
      "Epoch: 1628 Loss: 106.48639061163274\n",
      "Epoch: 1629 Loss: 106.48634182031962\n",
      "Epoch: 1630 Loss: 106.48629308665632\n",
      "Epoch: 1631 Loss: 106.48624441051585\n",
      "Epoch: 1632 Loss: 106.4861957917717\n",
      "Epoch: 1633 Loss: 106.48614723029775\n",
      "Epoch: 1634 Loss: 106.48609872596815\n",
      "Epoch: 1635 Loss: 106.48605027865743\n",
      "Epoch: 1636 Loss: 106.48600188824054\n",
      "Epoch: 1637 Loss: 106.48595355459267\n",
      "Epoch: 1638 Loss: 106.48590527758942\n",
      "Epoch: 1639 Loss: 106.48585705710687\n",
      "Epoch: 1640 Loss: 106.48580889302123\n",
      "Epoch: 1641 Loss: 106.48576078520924\n",
      "Epoch: 1642 Loss: 106.48571273354787\n",
      "Epoch: 1643 Loss: 106.48566473791452\n",
      "Epoch: 1644 Loss: 106.48561679818688\n",
      "Epoch: 1645 Loss: 106.48556891424306\n",
      "Epoch: 1646 Loss: 106.48552108596141\n",
      "Epoch: 1647 Loss: 106.4854733132207\n",
      "Epoch: 1648 Loss: 106.48542559590004\n",
      "Epoch: 1649 Loss: 106.4853779338789\n",
      "Epoch: 1650 Loss: 106.48533032703702\n",
      "Epoch: 1651 Loss: 106.48528277525452\n",
      "Epoch: 1652 Loss: 106.48523527841185\n",
      "Epoch: 1653 Loss: 106.48518783638985\n",
      "Epoch: 1654 Loss: 106.48514044906959\n",
      "Epoch: 1655 Loss: 106.48509311633256\n",
      "Epoch: 1656 Loss: 106.48504583806056\n",
      "Epoch: 1657 Loss: 106.48499861413576\n",
      "Epoch: 1658 Loss: 106.48495144444055\n",
      "Epoch: 1659 Loss: 106.4849043288578\n",
      "Epoch: 1660 Loss: 106.48485726727058\n",
      "Epoch: 1661 Loss: 106.48481025956235\n",
      "Epoch: 1662 Loss: 106.48476330561691\n",
      "Epoch: 1663 Loss: 106.48471640531834\n",
      "Epoch: 1664 Loss: 106.48466955855108\n",
      "Epoch: 1665 Loss: 106.48462276519986\n",
      "Epoch: 1666 Loss: 106.48457602514979\n",
      "Epoch: 1667 Loss: 106.48452933828626\n",
      "Epoch: 1668 Loss: 106.48448270449501\n",
      "Epoch: 1669 Loss: 106.48443612366202\n",
      "Epoch: 1670 Loss: 106.48438959567369\n",
      "Epoch: 1671 Loss: 106.48434312041667\n",
      "Epoch: 1672 Loss: 106.48429669777796\n",
      "Epoch: 1673 Loss: 106.48425032764486\n",
      "Epoch: 1674 Loss: 106.48420400990497\n",
      "Epoch: 1675 Loss: 106.48415774444629\n",
      "Epoch: 1676 Loss: 106.484111531157\n",
      "Epoch: 1677 Loss: 106.48406536992564\n",
      "Epoch: 1678 Loss: 106.4840192606411\n",
      "Epoch: 1679 Loss: 106.48397320319259\n",
      "Epoch: 1680 Loss: 106.48392719746955\n",
      "Epoch: 1681 Loss: 106.48388124336178\n",
      "Epoch: 1682 Loss: 106.48383534075934\n",
      "Epoch: 1683 Loss: 106.48378948955268\n",
      "Epoch: 1684 Loss: 106.48374368963245\n",
      "Epoch: 1685 Loss: 106.48369794088967\n",
      "Epoch: 1686 Loss: 106.48365224321567\n",
      "Epoch: 1687 Loss: 106.48360659650204\n",
      "Epoch: 1688 Loss: 106.4835610006407\n",
      "Epoch: 1689 Loss: 106.48351545552384\n",
      "Epoch: 1690 Loss: 106.48346996104397\n",
      "Epoch: 1691 Loss: 106.48342451709385\n",
      "Epoch: 1692 Loss: 106.48337912356662\n",
      "Epoch: 1693 Loss: 106.48333378035566\n",
      "Epoch: 1694 Loss: 106.48328848735467\n",
      "Epoch: 1695 Loss: 106.4832432444576\n",
      "Epoch: 1696 Loss: 106.4831980515587\n",
      "Epoch: 1697 Loss: 106.48315290855255\n",
      "Epoch: 1698 Loss: 106.483107815334\n",
      "Epoch: 1699 Loss: 106.48306277179822\n",
      "Epoch: 1700 Loss: 106.48301777784057\n",
      "Epoch: 1701 Loss: 106.48297283335677\n",
      "Epoch: 1702 Loss: 106.48292793824288\n",
      "Epoch: 1703 Loss: 106.48288309239513\n",
      "Epoch: 1704 Loss: 106.48283829571011\n",
      "Epoch: 1705 Loss: 106.48279354808462\n",
      "Epoch: 1706 Loss: 106.48274884941586\n",
      "Epoch: 1707 Loss: 106.4827041996012\n",
      "Epoch: 1708 Loss: 106.48265959853836\n",
      "Epoch: 1709 Loss: 106.48261504612535\n",
      "Epoch: 1710 Loss: 106.48257054226028\n",
      "Epoch: 1711 Loss: 106.48252608684182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1712 Loss: 106.48248167976872\n",
      "Epoch: 1713 Loss: 106.48243732094008\n",
      "Epoch: 1714 Loss: 106.48239301025525\n",
      "Epoch: 1715 Loss: 106.48234874761387\n",
      "Epoch: 1716 Loss: 106.48230453291579\n",
      "Epoch: 1717 Loss: 106.48226036606127\n",
      "Epoch: 1718 Loss: 106.4822162469507\n",
      "Epoch: 1719 Loss: 106.48217217548479\n",
      "Epoch: 1720 Loss: 106.4821281515646\n",
      "Epoch: 1721 Loss: 106.48208417509129\n",
      "Epoch: 1722 Loss: 106.48204024596645\n",
      "Epoch: 1723 Loss: 106.48199636409186\n",
      "Epoch: 1724 Loss: 106.48195252936954\n",
      "Epoch: 1725 Loss: 106.48190874170186\n",
      "Epoch: 1726 Loss: 106.48186500099136\n",
      "Epoch: 1727 Loss: 106.48182130714093\n",
      "Epoch: 1728 Loss: 106.48177766005368\n",
      "Epoch: 1729 Loss: 106.48173405963296\n",
      "Epoch: 1730 Loss: 106.48169050578238\n",
      "Epoch: 1731 Loss: 106.4816469984059\n",
      "Epoch: 1732 Loss: 106.48160353740762\n",
      "Epoch: 1733 Loss: 106.48156012269199\n",
      "Epoch: 1734 Loss: 106.48151675416364\n",
      "Epoch: 1735 Loss: 106.48147343172754\n",
      "Epoch: 1736 Loss: 106.48143015528883\n",
      "Epoch: 1737 Loss: 106.48138692475294\n",
      "Epoch: 1738 Loss: 106.48134374002561\n",
      "Epoch: 1739 Loss: 106.48130060101276\n",
      "Epoch: 1740 Loss: 106.48125750762058\n",
      "Epoch: 1741 Loss: 106.48121445975553\n",
      "Epoch: 1742 Loss: 106.48117145732428\n",
      "Epoch: 1743 Loss: 106.48112850023378\n",
      "Epoch: 1744 Loss: 106.4810855883913\n",
      "Epoch: 1745 Loss: 106.4810427217042\n",
      "Epoch: 1746 Loss: 106.4809999000802\n",
      "Epoch: 1747 Loss: 106.48095712342726\n",
      "Epoch: 1748 Loss: 106.48091439165357\n",
      "Epoch: 1749 Loss: 106.48087170466754\n",
      "Epoch: 1750 Loss: 106.48082906237781\n",
      "Epoch: 1751 Loss: 106.48078646469338\n",
      "Epoch: 1752 Loss: 106.48074391152338\n",
      "Epoch: 1753 Loss: 106.48070140277719\n",
      "Epoch: 1754 Loss: 106.4806589383645\n",
      "Epoch: 1755 Loss: 106.48061651819515\n",
      "Epoch: 1756 Loss: 106.48057414217931\n",
      "Epoch: 1757 Loss: 106.48053181022733\n",
      "Epoch: 1758 Loss: 106.4804895222498\n",
      "Epoch: 1759 Loss: 106.48044727815757\n",
      "Epoch: 1760 Loss: 106.48040507786172\n",
      "Epoch: 1761 Loss: 106.48036292127354\n",
      "Epoch: 1762 Loss: 106.48032080830464\n",
      "Epoch: 1763 Loss: 106.48027873886673\n",
      "Epoch: 1764 Loss: 106.48023671287191\n",
      "Epoch: 1765 Loss: 106.48019473023234\n",
      "Epoch: 1766 Loss: 106.48015279086056\n",
      "Epoch: 1767 Loss: 106.48011089466927\n",
      "Epoch: 1768 Loss: 106.4800690415714\n",
      "Epoch: 1769 Loss: 106.48002723148016\n",
      "Epoch: 1770 Loss: 106.4799854643089\n",
      "Epoch: 1771 Loss: 106.47994373997132\n",
      "Epoch: 1772 Loss: 106.47990205838123\n",
      "Epoch: 1773 Loss: 106.4798604194527\n",
      "Epoch: 1774 Loss: 106.47981882310012\n",
      "Epoch: 1775 Loss: 106.47977726923796\n",
      "Epoch: 1776 Loss: 106.47973575778101\n",
      "Epoch: 1777 Loss: 106.47969428864424\n",
      "Epoch: 1778 Loss: 106.4796528617429\n",
      "Epoch: 1779 Loss: 106.47961147699239\n",
      "Epoch: 1780 Loss: 106.47957013430836\n",
      "Epoch: 1781 Loss: 106.47952883360671\n",
      "Epoch: 1782 Loss: 106.47948757480353\n",
      "Epoch: 1783 Loss: 106.47944635781515\n",
      "Epoch: 1784 Loss: 106.47940518255812\n",
      "Epoch: 1785 Loss: 106.47936404894912\n",
      "Epoch: 1786 Loss: 106.47932295690521\n",
      "Epoch: 1787 Loss: 106.47928190634356\n",
      "Epoch: 1788 Loss: 106.47924089718154\n",
      "Epoch: 1789 Loss: 106.47919992933681\n",
      "Epoch: 1790 Loss: 106.4791590027272\n",
      "Epoch: 1791 Loss: 106.47911811727077\n",
      "Epoch: 1792 Loss: 106.47907727288575\n",
      "Epoch: 1793 Loss: 106.47903646949068\n",
      "Epoch: 1794 Loss: 106.47899570700422\n",
      "Epoch: 1795 Loss: 106.47895498534524\n",
      "Epoch: 1796 Loss: 106.47891430443293\n",
      "Epoch: 1797 Loss: 106.47887366418655\n",
      "Epoch: 1798 Loss: 106.47883306452567\n",
      "Epoch: 1799 Loss: 106.47879250537004\n",
      "Epoch: 1800 Loss: 106.47875198663958\n",
      "Epoch: 1801 Loss: 106.47871150825449\n",
      "Epoch: 1802 Loss: 106.47867107013508\n",
      "Epoch: 1803 Loss: 106.478630672202\n",
      "Epoch: 1804 Loss: 106.47859031437598\n",
      "Epoch: 1805 Loss: 106.47854999657801\n",
      "Epoch: 1806 Loss: 106.47850971872931\n",
      "Epoch: 1807 Loss: 106.47846948075124\n",
      "Epoch: 1808 Loss: 106.47842928256543\n",
      "Epoch: 1809 Loss: 106.47838912409361\n",
      "Epoch: 1810 Loss: 106.47834900525791\n",
      "Epoch: 1811 Loss: 106.4783089259804\n",
      "Epoch: 1812 Loss: 106.47826888618351\n",
      "Epoch: 1813 Loss: 106.47822888578995\n",
      "Epoch: 1814 Loss: 106.47818892472245\n",
      "Epoch: 1815 Loss: 106.47814900290396\n",
      "Epoch: 1816 Loss: 106.47810912025777\n",
      "Epoch: 1817 Loss: 106.47806927670723\n",
      "Epoch: 1818 Loss: 106.47802947217598\n",
      "Epoch: 1819 Loss: 106.47798970658775\n",
      "Epoch: 1820 Loss: 106.47794997986658\n",
      "Epoch: 1821 Loss: 106.4779102919366\n",
      "Epoch: 1822 Loss: 106.47787064272224\n",
      "Epoch: 1823 Loss: 106.47783103214807\n",
      "Epoch: 1824 Loss: 106.47779146013887\n",
      "Epoch: 1825 Loss: 106.47775192661953\n",
      "Epoch: 1826 Loss: 106.47771243151523\n",
      "Epoch: 1827 Loss: 106.47767297475136\n",
      "Epoch: 1828 Loss: 106.4776335562534\n",
      "Epoch: 1829 Loss: 106.47759417594709\n",
      "Epoch: 1830 Loss: 106.47755483375836\n",
      "Epoch: 1831 Loss: 106.4775155296133\n",
      "Epoch: 1832 Loss: 106.47747626343818\n",
      "Epoch: 1833 Loss: 106.4774370351595\n",
      "Epoch: 1834 Loss: 106.47739784470394\n",
      "Epoch: 1835 Loss: 106.47735869199835\n",
      "Epoch: 1836 Loss: 106.47731957696975\n",
      "Epoch: 1837 Loss: 106.47728049954542\n",
      "Epoch: 1838 Loss: 106.47724145965269\n",
      "Epoch: 1839 Loss: 106.4772024572192\n",
      "Epoch: 1840 Loss: 106.47716349217275\n",
      "Epoch: 1841 Loss: 106.4771245644413\n",
      "Epoch: 1842 Loss: 106.47708567395294\n",
      "Epoch: 1843 Loss: 106.47704682063609\n",
      "Epoch: 1844 Loss: 106.47700800441919\n",
      "Epoch: 1845 Loss: 106.47696922523097\n",
      "Epoch: 1846 Loss: 106.47693048300029\n",
      "Epoch: 1847 Loss: 106.47689177765618\n",
      "Epoch: 1848 Loss: 106.47685310912794\n",
      "Epoch: 1849 Loss: 106.47681447734493\n",
      "Epoch: 1850 Loss: 106.47677588223678\n",
      "Epoch: 1851 Loss: 106.47673732373318\n",
      "Epoch: 1852 Loss: 106.47669880176414\n",
      "Epoch: 1853 Loss: 106.47666031625978\n",
      "Epoch: 1854 Loss: 106.47662186715041\n",
      "Epoch: 1855 Loss: 106.4765834543665\n",
      "Epoch: 1856 Loss: 106.47654507783865\n",
      "Epoch: 1857 Loss: 106.47650673749777\n",
      "Epoch: 1858 Loss: 106.47646843327482\n",
      "Epoch: 1859 Loss: 106.47643016510094\n",
      "Epoch: 1860 Loss: 106.47639193290752\n",
      "Epoch: 1861 Loss: 106.47635373662608\n",
      "Epoch: 1862 Loss: 106.47631557618828\n",
      "Epoch: 1863 Loss: 106.47627745152604\n",
      "Epoch: 1864 Loss: 106.47623936257136\n",
      "Epoch: 1865 Loss: 106.47620130925642\n",
      "Epoch: 1866 Loss: 106.47616329151364\n",
      "Epoch: 1867 Loss: 106.47612530927553\n",
      "Epoch: 1868 Loss: 106.47608736247486\n",
      "Epoch: 1869 Loss: 106.47604945104446\n",
      "Epoch: 1870 Loss: 106.47601157491738\n",
      "Epoch: 1871 Loss: 106.47597373402687\n",
      "Epoch: 1872 Loss: 106.47593592830626\n",
      "Epoch: 1873 Loss: 106.47589815768919\n",
      "Epoch: 1874 Loss: 106.47586042210932\n",
      "Epoch: 1875 Loss: 106.47582272150052\n",
      "Epoch: 1876 Loss: 106.4757850557969\n",
      "Epoch: 1877 Loss: 106.47574742493262\n",
      "Epoch: 1878 Loss: 106.47570982884204\n",
      "Epoch: 1879 Loss: 106.47567226745977\n",
      "Epoch: 1880 Loss: 106.47563474072044\n",
      "Epoch: 1881 Loss: 106.47559724855898\n",
      "Epoch: 1882 Loss: 106.47555979091037\n",
      "Epoch: 1883 Loss: 106.47552236770983\n",
      "Epoch: 1884 Loss: 106.4754849788927\n",
      "Epoch: 1885 Loss: 106.47544762439446\n",
      "Epoch: 1886 Loss: 106.47541030415087\n",
      "Epoch: 1887 Loss: 106.47537301809767\n",
      "Epoch: 1888 Loss: 106.4753357661709\n",
      "Epoch: 1889 Loss: 106.47529854830668\n",
      "Epoch: 1890 Loss: 106.47526136444134\n",
      "Epoch: 1891 Loss: 106.47522421451131\n",
      "Epoch: 1892 Loss: 106.47518709845326\n",
      "Epoch: 1893 Loss: 106.47515001620391\n",
      "Epoch: 1894 Loss: 106.47511296770027\n",
      "Epoch: 1895 Loss: 106.4750759528794\n",
      "Epoch: 1896 Loss: 106.4750389716785\n",
      "Epoch: 1897 Loss: 106.47500202403504\n",
      "Epoch: 1898 Loss: 106.47496510988654\n",
      "Epoch: 1899 Loss: 106.47492822917073\n",
      "Epoch: 1900 Loss: 106.47489138182546\n",
      "Epoch: 1901 Loss: 106.47485456778873\n",
      "Epoch: 1902 Loss: 106.47481778699873\n",
      "Epoch: 1903 Loss: 106.4747810393938\n",
      "Epoch: 1904 Loss: 106.47474432491236\n",
      "Epoch: 1905 Loss: 106.47470764349306\n",
      "Epoch: 1906 Loss: 106.47467099507473\n",
      "Epoch: 1907 Loss: 106.47463437959624\n",
      "Epoch: 1908 Loss: 106.47459779699663\n",
      "Epoch: 1909 Loss: 106.47456124721522\n",
      "Epoch: 1910 Loss: 106.47452473019132\n",
      "Epoch: 1911 Loss: 106.47448824586442\n",
      "Epoch: 1912 Loss: 106.47445179417429\n",
      "Epoch: 1913 Loss: 106.47441537506069\n",
      "Epoch: 1914 Loss: 106.47437898846358\n",
      "Epoch: 1915 Loss: 106.47434263432311\n",
      "Epoch: 1916 Loss: 106.4743063125795\n",
      "Epoch: 1917 Loss: 106.47427002317318\n",
      "Epoch: 1918 Loss: 106.47423376604468\n",
      "Epoch: 1919 Loss: 106.47419754113471\n",
      "Epoch: 1920 Loss: 106.47416134838411\n",
      "Epoch: 1921 Loss: 106.47412518773386\n",
      "Epoch: 1922 Loss: 106.47408905912513\n",
      "Epoch: 1923 Loss: 106.47405296249913\n",
      "Epoch: 1924 Loss: 106.47401689779731\n",
      "Epoch: 1925 Loss: 106.47398086496125\n",
      "Epoch: 1926 Loss: 106.4739448639326\n",
      "Epoch: 1927 Loss: 106.47390889465322\n",
      "Epoch: 1928 Loss: 106.4738729570651\n",
      "Epoch: 1929 Loss: 106.47383705111037\n",
      "Epoch: 1930 Loss: 106.4738011767313\n",
      "Epoch: 1931 Loss: 106.47376533387026\n",
      "Epoch: 1932 Loss: 106.47372952246982\n",
      "Epoch: 1933 Loss: 106.4736937424727\n",
      "Epoch: 1934 Loss: 106.47365799382166\n",
      "Epoch: 1935 Loss: 106.47362227645968\n",
      "Epoch: 1936 Loss: 106.4735865903299\n",
      "Epoch: 1937 Loss: 106.47355093537551\n",
      "Epoch: 1938 Loss: 106.47351531153991\n",
      "Epoch: 1939 Loss: 106.47347971876658\n",
      "Epoch: 1940 Loss: 106.47344415699921\n",
      "Epoch: 1941 Loss: 106.4734086261816\n",
      "Epoch: 1942 Loss: 106.47337312625757\n",
      "Epoch: 1943 Loss: 106.47333765717127\n",
      "Epoch: 1944 Loss: 106.47330221886688\n",
      "Epoch: 1945 Loss: 106.47326681128868\n",
      "Epoch: 1946 Loss: 106.47323143438113\n",
      "Epoch: 1947 Loss: 106.47319608808888\n",
      "Epoch: 1948 Loss: 106.47316077235665\n",
      "Epoch: 1949 Loss: 106.47312548712924\n",
      "Epoch: 1950 Loss: 106.47309023235167\n",
      "Epoch: 1951 Loss: 106.47305500796908\n",
      "Epoch: 1952 Loss: 106.4730198139267\n",
      "Epoch: 1953 Loss: 106.4729846501699\n",
      "Epoch: 1954 Loss: 106.47294951664428\n",
      "Epoch: 1955 Loss: 106.47291441329541\n",
      "Epoch: 1956 Loss: 106.47287934006911\n",
      "Epoch: 1957 Loss: 106.47284429691126\n",
      "Epoch: 1958 Loss: 106.47280928376793\n",
      "Epoch: 1959 Loss: 106.47277430058523\n",
      "Epoch: 1960 Loss: 106.47273934730956\n",
      "Epoch: 1961 Loss: 106.47270442388722\n",
      "Epoch: 1962 Loss: 106.47266953026482\n",
      "Epoch: 1963 Loss: 106.47263466638907\n",
      "Epoch: 1964 Loss: 106.47259983220673\n",
      "Epoch: 1965 Loss: 106.47256502766479\n",
      "Epoch: 1966 Loss: 106.47253025271023\n",
      "Epoch: 1967 Loss: 106.47249550729032\n",
      "Epoch: 1968 Loss: 106.47246079135229\n",
      "Epoch: 1969 Loss: 106.47242610484366\n",
      "Epoch: 1970 Loss: 106.47239144771196\n",
      "Epoch: 1971 Loss: 106.47235681990485\n",
      "Epoch: 1972 Loss: 106.47232222137019\n",
      "Epoch: 1973 Loss: 106.4722876520559\n",
      "Epoch: 1974 Loss: 106.47225311191005\n",
      "Epoch: 1975 Loss: 106.47221860088075\n",
      "Epoch: 1976 Loss: 106.4721841189164\n",
      "Epoch: 1977 Loss: 106.47214966596543\n",
      "Epoch: 1978 Loss: 106.47211524197633\n",
      "Epoch: 1979 Loss: 106.4720808468978\n",
      "Epoch: 1980 Loss: 106.47204648067864\n",
      "Epoch: 1981 Loss: 106.47201214326779\n",
      "Epoch: 1982 Loss: 106.47197783461424\n",
      "Epoch: 1983 Loss: 106.47194355466716\n",
      "Epoch: 1984 Loss: 106.4719093033759\n",
      "Epoch: 1985 Loss: 106.47187508068973\n",
      "Epoch: 1986 Loss: 106.47184088655824\n",
      "Epoch: 1987 Loss: 106.47180672093108\n",
      "Epoch: 1988 Loss: 106.47177258375798\n",
      "Epoch: 1989 Loss: 106.47173847498885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1990 Loss: 106.47170439457365\n",
      "Epoch: 1991 Loss: 106.47167034246246\n",
      "Epoch: 1992 Loss: 106.47163631860558\n",
      "Epoch: 1993 Loss: 106.47160232295329\n",
      "Epoch: 1994 Loss: 106.4715683554561\n",
      "Epoch: 1995 Loss: 106.47153441606454\n",
      "Epoch: 1996 Loss: 106.47150050472936\n",
      "Epoch: 1997 Loss: 106.47146662140133\n",
      "Epoch: 1998 Loss: 106.4714327660314\n",
      "Epoch: 1999 Loss: 106.4713989385706\n",
      "machine [ 0.69769387 -1.20489514  1.46220535  0.33145503 -0.34882228  0.87373767\n",
      "  0.17934263  0.5587321  -0.29495699 -0.96996388]\n",
      "machine 1.0000000000000002\n",
      "exciting 0.3281478081280282\n",
      "natural 0.22028543753673735\n"
     ]
    }
   ],
   "source": [
    "w2v = word2vec()\n",
    "\n",
    "# Numpy ndarray with one-hot representation for [target_word, context_words]\n",
    "training_data = w2v.generate_training_data(corpus)\n",
    "\n",
    "# Training\n",
    "w2v.train(training_data)\n",
    "\n",
    "# Get vector for word\n",
    "word = \"machine\"\n",
    "vec = w2v.word_vec(word)\n",
    "print(word, vec)\n",
    "\n",
    "# Find similar words\n",
    "w2v.vec_sim(\"machine\", 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
