18:02:25	 From Michal Fabinger : If you are joining us for the first time, here is some useful information:

We started earlier this week, but hopefully, it will not be too hard to catch up. We need our participants to have Python and Jupyter Lab installed. We recommend the Anaconda installation (with Python 3.7).

https://www.anaconda.com/distribution/

Later we will need PyTorch and TensorFlow. Installing PyTorch is easy using these directions:

https://pytorch.org/

TensorFlow is just a little bit more complicated:

https://www.tensorflow.org/install/

We have covered some material from the Deep Learning book (http://www.deeplearningbook.org/ , free online), namely sections 4.3 (before 4.3.1), 8.1.3, and 8.3.1.
18:02:53	 From Michal Fabinger : Our school's website is 

https://tokyodatascience.com/

and some useful pages are:

https://tokyodatascience.com/courses
https://tokyodatascience.com/deeplearning
https://tokyodatascience.com/datascience
https://tokyodatascience.com/tuition
https://tokyodatascience.com/community

The present class corresponds to a combined Deep Learning and Data Science track. We will cover all of the Deep Learning topics and all the important extra topics from the Data Science curriculum.
18:03:08	 From Michal Fabinger : In today's class we will be using the optimizer slides

https://www.dropbox.com/s/w1k61x16dbhbklw/OptimizerSlides.pdf?dl=0

and the Jupyter notebook

https://www.dropbox.com/s/gkxn2gj2d72y0iv/linear_model.ipynb?dl=0

which we will modify to implement different optimizers. 

If you have trouble with the notebooks animation writing functions, you may need to install ffmpeg for this to work correctly. For Windows users, the following links may be helpful:https://www.wikihow.com/Install-FFmpeg-on-Windows https://www.youtube.com/watch?v=pHR3ttH5t-w  
For Mac users, the following link works:https://github.com/fluent-ffmpeg/node-fluent-ffmpeg/wiki/Installing-ffmpeg-on-Mac-OS-X
18:15:16	 From akuaAxAyyAtIzEL6wr3VH95jznH2vTlVtRQhh5VIMwZch1fKpDATWtgc0+ip+XdpEjJli+yk3XTEWMK/9eAFZA== : M_ please mute
18:15:20	 From paulmaddela : Somebody has to mute their mic
18:15:48	 From akuaAxAyyAtIzEL6wr3VH95jznH2vTlVtRQhh5VIMwZch1fKpDATWtgc0+ip+XdpEjJli+yk3XTEWMK/9eAFZA== : Irina please mute
18:15:48	 From Doug Chang : We have 2 mics open causing feedback
18:15:51	 From Doug Chang : cant hear
18:21:22	 From Doug Chang : does this prevent vanishing gradient problem where small number * small number = smaller number?
18:44:14	 From Atiq : how can I access the slides for earlier classes?
18:44:46	 From Atiq : (especially first few classes)
18:45:14	 From Irina Max : it is a book, link  at meetup page 
18:45:56	 From Irina Max : https://www.deeplearningbook.org/
18:46:05	 From Atiq : ok. Thanks
18:58:59	 From akuaAxAyyAtIzEL6wr3VH95jznH2vTlVtRQhh5VIMwZch1fKpDATWtgc0+ip+XdpEjJli+yk3XTEWMK/9eAFZA== : Irina can you mute
18:59:39	 From Doug Chang : Irina can you mute
18:59:42	 From Doug Chang : we cant hear
19:07:37	 From Prafful : Are there any heuristics with regards to choosing when to use which of these optimizers?
19:11:03	 From Md Imbesat Hassan Rizvi : I guess there was a paper SGD although sow than Adam is still the best in case of generalization
19:13:11	 From Prafful : Ok, thanks!
19:17:06	 From Richard : better?analytical methods likely work well with few dimensions
19:19:46	 From Prafful : It would be good to have a look at EDA as well, it would be great even if you can point to useful resources as most of the class seems to be working on it already so might be redundant for them
19:20:52	 From Doug Chang : There are 4 people looking for partners/projects on slack
19:20:56	 From Md Imbesat Hassan Rizvi : Is there a list of project proposal somewhere? I couldn't join yesterday so would be helpful if it is already documented
19:21:00	 From Doug Chang : One is in the Europe timezone
19:23:10	 From Prafful : Ok, sounds good
19:24:33	 From paulmaddela : Hey Michal you also once  said that causal inference could be useful for data science
19:36:38	 From Salil Mishra : I am working on a local competition on predicting loan_defaults, if anyone could help share some insights or has worked with this problem before that would be great
19:39:41	 From Michal Fabinger : Gradient descent
Gradient descent with momentum
Gradient descent with Nesterov momentum
AdaGrad
Adam
19:43:47	 From Irina Max : Learning Rate Decay ??
19:45:07	 From Doug Chang : Hangzhou City Brain
19:45:19	 From Doug Chang : yes
19:45:30	 From Doug Chang : yes
19:45:32	 From Doug Chang : alibaba
19:45:35	 From Md Imbesat Hassan Rizvi : Well, have to check it now. :-)
19:45:44	 From Doug Chang : Reduced traffic jams
19:45:55	 From Doug Chang : Always room for improvement
19:47:21	 From Atiq : What will be the topic for next class?
19:47:27	 From Atiq : can we get slides ahead ?
19:48:32	 From Ernesto Castillo : yes
19:48:38	 From Irina Max : I share it on Slack
19:48:43	 From abhishek : We can derive one
19:49:23	 From Irina Max : yes
19:49:33	 From Irina Max : return alpha + beta*x*x + gamma
19:49:54	 From Doug Chang : The derivatives have to exist
19:50:42	 From Doug Chang : can we replace loss w/cross entropy vs. linear and get classification?
19:51:27	 From abhishek : yes I think so we can
