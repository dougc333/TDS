{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-436be9a3b3c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "n_samples = 50\n",
    "X = np.linspace(1, 50, n_samples)\n",
    "Y = 10*X + 6 + 2*np.random.randn(n_samples)\n",
    "print(Y.shape,X.shape)\n",
    "\n",
    "plt.plot(X, Y, 'k.')\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(param_list):\n",
    "    w, b = param_list\n",
    "    pred = w*X+b\n",
    "    return np.sqrt(((pred - Y) ** 2).mean(axis=None))/(2*len(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at w=0.0, b=0.0 is: 2.9809044649485408\n",
      "Cost at w=10.0, b=4.0 is: 0.03204794719387623\n",
      "Gradient at w=0.0, b=0.0 is: [array(-0.29297047), array(-0.00876516)]\n",
      "Gradient at w=10.0, b=4.0 is: [array(-0.14406455), array(-0.00711783)]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import math\n",
    "from autograd import grad\n",
    "\n",
    "\n",
    "# Cost of w=0, b=0\n",
    "w, b = 0., 0.\n",
    "print(\"Cost at w={}, b={} is: {}\".format(w, b, cost([w, b])))\n",
    "\n",
    "# Cost of w=10, b=4. Should be lower than w=0, b=0\n",
    "w, b = 10., 4.\n",
    "print(\"Cost at w={}, b={} is: {}\".format(w, b, cost([w, b])))\n",
    "\n",
    "# Computing the gradient at w=0, b=0\n",
    "\n",
    "grad_cost =grad(cost)\n",
    "w, b = 0., 0.\n",
    "print(\"Gradient at w={}, b={} is: {}\".format(w, b, grad_cost([w, b])))\n",
    "\n",
    "# Computing the gradient at w=10, b=4. We would expect it to be smaller than at 0, 0\n",
    "w, b = 10., 4.\n",
    "print(\"Gradient at w={}, b={} is: {}\".format(w, b, grad_cost([w, b])))\n",
    "\n",
    "\n",
    "\n",
    "def adagrad_gd(param_init, cost, niter=5, lr=1e-2, eps=1e-8, random_seed=0):\n",
    "    \"\"\"\n",
    "    param_init: List of initial values of parameters\n",
    "    cost: cost function\n",
    "    niter: Number of iterations to run\n",
    "    lr: Learning rate\n",
    "    eps: Fudge factor, to avoid division by zero\n",
    "    \"\"\"\n",
    "    # Function to compute the gradient of the cost function\n",
    "    grad_cost = grad(cost)\n",
    "    params = deepcopy(param_init)\n",
    "    param_array, grad_array, lr_array, cost_array = [params], [], [[lr for _ in params]], [cost(params)]\n",
    "    # Initialising sum of squares of gradients for each param as 0\n",
    "    sum_squares_gradients = [np.zeros_like(param) for param in params]\n",
    "    for i in range(4):\n",
    "        out_params = []\n",
    "        gradients = grad_cost(params)\n",
    "        print(\"gradients:\",gradients,\" type:\",type(gradients[0]),type(gradients[1]),gradients[0].shape,gradients[1].shape)\n",
    "        # At each iteration, we add the square of the gradients to `sum_squares_gradients`\n",
    "        sum_squares_gradients= [eps + sum_prev + np.square(g) for sum_prev, g in zip(sum_squares_gradients, gradients)]\n",
    "        print(\"sum_squares_gradients:\",sum_squares_gradients)\n",
    "        # Adapted learning rate for parameter list\n",
    "        lrs = [np.divide(lr, np.sqrt(sg)) for sg in sum_squares_gradients]\n",
    "        print(\"lrs:\",lrs)\n",
    "        # Paramter update\n",
    "        params = [param-(adapted_lr*grad_param) for param, adapted_lr, grad_param in zip(params, lrs, gradients)]\n",
    "        print(\"params:\",params)\n",
    "        param_array.append(params)\n",
    "        lr_array.append(lrs)\n",
    "        grad_array.append(gradients)\n",
    "        cost_array.append(cost(params))\n",
    "        \n",
    "    return params, param_array, grad_array, lr_array, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_param: 0.06651722238316789  second_param: 0.3024718977397814\n",
      "gradients: [array(-0.29297186), array(-0.0087632)]  type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> () ()\n",
      "sum_squares_gradients: [0.08583252313062349, 7.680360467210906e-05]\n",
      "lrs: [0.03413296853458938, 1.1410618834373025]\n",
      "params: [0.0765172218006381, 0.31247124670747833]\n",
      "gradients: [array(-0.29297186), array(-0.00876318)]  type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> () ()\n",
      "sum_squares_gradients: [0.1716650460212332, 0.0001536068633469537]\n",
      "lrs: [0.024135653529707857, 0.8068535042434689]\n",
      "params: [0.08358828919564959, 0.319541846204688]\n",
      "gradients: [array(-0.29297186), array(-0.00876316)]  type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> () ()\n",
      "sum_squares_gradients: [0.25749756873472707, 0.00023040987693335538]\n",
      "lrs: [0.019706678596832297, 0.6587937252260828]\n",
      "params: [0.08936179154456016, 0.3253149625444586]\n",
      "gradients: [array(-0.29297186), array(-0.00876315)]  type: <class 'numpy.ndarray'> <class 'numpy.ndarray'> () ()\n",
      "sum_squares_gradients: [0.3433300912990426, 0.00030721269014007943]\n",
      "lrs: [0.017066484297702765, 0.5705325467775668]\n",
      "params: [0.09436179124570941, 0.3303146253292988]\n"
     ]
    }
   ],
   "source": [
    "# Fixing the random seed for reproducible init params for `W` and `b`\n",
    "first_param = np.random.randn()\n",
    "second_param = np.random.randn()\n",
    "print(\"first_param:\",first_param, \" second_param:\",second_param)\n",
    "param_init = [first_param, second_param]\n",
    "lr = 0.01\n",
    "eps=1e-8\n",
    "niter=1000\n",
    "ada_params, ada_param_array, ada_grad_array, ada_lr_array, ada_cost_array = adagrad_gd(param_init, cost, niter=niter, lr=lr, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ada_cost_array, name='Cost').plot(title='Adagrad: Cost v/s # Iterations')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"# Iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
