{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Derivatives</h6>\n",
    "Why do we compute backward version of partial derivatives vs. forward? \n",
    "<img src=\"files/forward_vs_backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Gradient Descent</h6>\n",
    "<p>the loss function is the average of ALL gradients!!! that is a lot of commpuation for each \n",
    "data point especially if you have large training sets.</p>\n",
    "<p>Research says if you use larger minibatches you may be overfitting more</p>\n",
    "The problem with computing gradient descent is there is too much computation to process the entire training\n",
    "set for each iteration. To simplify we can use batches which if picked evenly and randomly should be the blue line. \n",
    "A simpler approach is SGD which picks one sample from the training set but you get random variation in direction as\n",
    "you see below because you are no longer averaging gradients. \n",
    "<img src=\"files/batch_vs_sgd.png\"/ height=300 width=300 \"\">\n",
    "<p>src:https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_flat.pdf</p>\n",
    "To fix the SGD problem where the gradient vector is pointing in random directions \n",
    "we add a momentum vector or velocity which is the moving average of the past gradients multiplied\n",
    "by a .9(momentum) term. \n",
    "<img src=\"files/sgd_ig.png\" />\n",
    "<p>How to visualize momentum? From the slide above SGD with momentum. \n",
    "Assume you are in a snow storm and you are lost and cant see how to get down the mountain w/skis? \n",
    "If you can see in all directions you can compute the gradient by looking everywhere which is what gradient\n",
    "descent does. If you are blinded in a snowstorm you can point the skis downhill and take small steps which is like\n",
    "what momentum does. \n",
    "If you can see in the snowstorm like 10ft whow would you modify SGD? \n",
    "Right now you have to average all of them in the area near you and you have to see. \n",
    "If you want to use gradient estimate you have to pick a couple locations in the 10ft circle around you and average\n",
    "those which is not any better than any of the previous schemes we tried. \n",
    "How to modify the equations to make it faster to compute by modifying the SGD equations? Average the\n",
    "past gradients and use the new training sample to change the direction by a smaller amount so we reduce\n",
    "the change of the gradient direction. Nesterov picks an area ahead of you and calculates the gradient from\n",
    "the new point 10ft away.</p>\n",
    "<img src=\"files/pol_vs_nesterov.png\" />\n",
    "For convex functions nesterov is 1/T**2 faster than GD! Only applies for purely convex which may not be the case\n",
    "in DL. \n",
    "<img src=\"files/nest.png\">\n",
    "After few iterations convergence behavior of GD algos\n",
    "<img src=\"files/mom_conv.png\" height=300 width=300 />\n",
    "Final iteration\n",
    "<img src=\"files/adagrad_conv.png\" height=300 width=300 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
