{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Given a set of data we are trying to fit a model. \n",
    "\n",
    "For supervised learning we are minimizing empirical risk \n",
    "empirical risk is how well you fit data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Derivatives</h6>\n",
    "Talks: Nathan Stratzrow @Berkeley Simons Institute. There should be a YouTube video.  \n",
    "Slides by Matthew Johnson gave talk on deep learnging summer School in tioronto 2017. \n",
    "\n",
    "The partial derivatives of the loss are the Jacobian:     \n",
    "<img src=\"files/jac.png\" />\n",
    "\n",
    "Why do we compute backward version of partial derivatives vs. forward? \n",
    "<img src=\"files/forward_vs_backward.png\" />\n",
    "Autograd is pytorch. TF uses autograd in 2.0 but uses graph computation for derivative. Which is better and under\n",
    "what conditions? \n",
    "<p>\n",
    "Autograd will take abs(x) and reduce to x because at x=7 it knows it is positive and so replaces it w/x. \n",
    "\n",
    "Autodiff in general will look at abs(x) as a PWL function w/2 cases. +/- 1 for derrivative of abs(x). \n",
    "\n",
    "When can autograd go bad? Example is abs(x) where you have if statement if x<0 or x>9. What can go wrong w/if statements. What can go wrong w/ if statements. The problem is if you have if statements then you have to be prepared for each one which is 2^100 if you have 100 if statements. You cant evaluate any of these. Becomes exponentially difficult. Want to use autograd vs. autodiff. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Gradient Descent</h6>\n",
    "<p>the loss function is the average of ALL gradients!!! that is a lot of commpuation for each \n",
    "data point especially if you have large training sets.</p>\n",
    "<p>Research says if you use larger minibatches you may be overfitting more</p>\n",
    "The problem with computing gradient descent is there is too much computation to process the entire training\n",
    "set for each iteration. To simplify we can use batches which if picked evenly and randomly should be the blue line. \n",
    "A simpler approach is SGD which picks one sample from the training set but you get random variation in direction as\n",
    "you see below because you are no longer averaging gradients. \n",
    "<img src=\"files/batch_vs_sgd.png\"/ height=300 width=300 \"\">\n",
    "<p>src:https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_flat.pdf</p>\n",
    "To fix the SGD problem where the gradient vector is pointing in random directions \n",
    "we add a momentum vector or velocity which is the moving average of the past gradients multiplied\n",
    "by a .9(momentum) term. \n",
    "<img src=\"files/sgd_ig.png\" />\n",
    "<p>How to visualize momentum? From the slide above SGD with momentum. \n",
    "Assume you are in a snow storm and you are lost and cant see how to get down the mountain w/skis? \n",
    "If you can see in all directions you can compute the gradient by looking everywhere which is what gradient\n",
    "descent does. If you are blinded in a snowstorm you can point the skis downhill and take small steps which is like\n",
    "what momentum does. \n",
    "If you can see in the snowstorm like 10ft whow would you modify SGD? \n",
    "Right now you have to average all of them in the area near you and you have to see. \n",
    "If you want to use gradient estimate you have to pick a couple locations in the 10ft circle around you and average\n",
    "those which is not any better than any of the previous schemes we tried. \n",
    "How to modify the equations to make it faster to compute by modifying the SGD equations? Average the\n",
    "past gradients and use the new training sample to change the direction by a smaller amount so we reduce\n",
    "the change of the gradient direction. Nesterov picks an area ahead of you and calculates the gradient from\n",
    "the new point 10ft away.</p>\n",
    "<img src=\"files/pol_vs_nesterov.png\" />\n",
    "For convex functions nesterov is 1/T**2 faster than GD! Only applies for purely convex which may not be the case\n",
    "in DL. Nesterov not used for DL. Is there a way to get it to work better for deep learning? \n",
    "<img src=\"files/nest.png\">\n",
    "Adagrad: Different dimensions of problem space have different meaning and scale. \n",
    "If 1 dimension of population stats is number of ppl in household and another dim is annual income, \n",
    "another dim location, average Jan temp, many different variables and dimensions. \n",
    "It may be if you compute the gradient the components of the gradient will be fairly large for 1dim \n",
    "but really really small for another dim. If you calculate standerd update you will get 0 for the vector \n",
    "or really tiny numbers. How do you fix this? \n",
    "You need to move closer to global  min how do you know if you go positive or negative if the gradient is close to 0. \n",
    "Some components will be close to 0. You move in tiny steps. How do you move? \n",
    "If the numbers are so close to 0 how do you adjust them? \n",
    "The problem is if the numbers are too small you arenâ€™t moving. \n",
    "We need to amplify the smaller numbers so we actually we move in all dimensions and make the bigger values smaller. \n",
    "Adagrad does this by scaling down the largest gradients and making the smaller gradients larger proportional to the\n",
    "inverse of the square root of the sum of the past gradients. \n",
    "\n",
    "<img src=\"files/ada.png\">\n",
    "RMSProp was suggested by Hinton in a coursera class and many papers refer to the Coursera class as a reference! RMS modifies\n",
    "Adagrad by changing the gradient accumulation from a sum of past gradients to a exponentially weighted average. RMSProp performs\n",
    "simulated annealing which means as it gets closer to the minima it reduces the learning rate to avoid the final oscillations\n",
    "around the mimima. \n",
    "<img src=\"files/rms.png\">\n",
    "\n",
    "\n",
    "<img src=\"files/adam.png\">\n",
    "\n",
    "After few iterations convergence behavior of GD algos\n",
    "<img src=\"files/mom_conv.png\" height=300 width=300 />\n",
    "Final iteration\n",
    "<img src=\"files/adagrad_conv.png\" height=300 width=300 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Loss Functions</h6>\n",
    "MSE loss is a good choice because it is convex and differentiable. For linear regression. \n",
    "When differetiating we are taking gradients which is in the up direction and we travel in the opposite direction. \n",
    "<img src=\"files/norm.png\" />\n",
    "<img src=\"files/huber.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you replace loss with \n",
    "$loss = loss+\\sum {a_i^2}$ for L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Activation Functions</h6>\n",
    "<img src=\"files/leakyrelu.png\" />\n",
    "Leaky relu not really adopted by community, we do see tanh. \n",
    "<h6>Validation error</h6>\n",
    "why does validation error go up when we train for a long time? Overfitting. \n",
    "<img src=\"files/valid.png\" height=300 width=300 />\n",
    "If you find Validation loss is bigger than training loss. Does this mean overfitting? Does this mean you should increase\n",
    "size of regularization. Natural for validation loss to be less than training loss. his terminology is training loss is \n",
    "same as test loss per Ng. If you see validation loss is large compared to trainign loss then this is overfitting. \n",
    "Before we fit network well as long as validation loss is going down then you are not overfitting. Early stopping is a \n",
    "form of regularization. Ng doesnt use early stopping. He wants to see the effect of regularization and you wouldnt see\n",
    "the effect of higher validation loss. Validation error wouldnt blow up. Dropout to reduce overfittng: select random\n",
    "    number of neurons and turn some off; turn off different ones on each optimization step. \n",
    "<p></p>\n",
    "can you ahve overfitting in linear regression? yes. if you have more coefficients than data. So if you just minimize MSE\n",
    "you can get values for coefficients that dont make sense. We can also have this in colinearity or correlated features. If \n",
    "you have polynomial fit like we did in hw we can have overfitting if the polynomial is more complex than the data. \n",
    "how do we get rid of overfitting? we can add l2 term called ridge regression in addition to MSE. Elastic loss\n",
    "<img src=\"files/elastic.png\"> advantage of L1 and L2 regularization. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>Review</h6>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
